{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "from pre_process import pre_process_data\n",
    "from ridge_regression_helpers import *\n",
    "from cross_validation import cross_validation_jet\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c33a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "y, x, ids = load_csv_data('train.csv')\n",
    "_, x_test, ids_test = load_csv_data('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9ae6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30), (250000,), (568238, 30), (568238,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, x.shape, ids.shape, x_test.shape, ids_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962745b7",
   "metadata": {},
   "source": [
    "# Step 2 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305783f",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39aa01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # print w and loss\n",
    "        #print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ab2b1",
   "metadata": {},
   "source": [
    "### Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc58ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial w\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        #print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748c585",
   "metadata": {},
   "source": [
    "### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95297065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517708b7",
   "metadata": {},
   "source": [
    "### Ridge regression using normal equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9067e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "\n",
    "    x_t = tx.T\n",
    "    lambd = lambda_ * 2 * len(y)\n",
    "    w = np.linalg.solve (np.dot(x_t, tx) + lambd * np.eye(tx.shape[1]), np.dot(x_t,y)) \n",
    "    loss = compute_mse(y, tx, w)\n",
    "\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b86368",
   "metadata": {},
   "source": [
    "### Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a651453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    max_iter = 1200\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c27140",
   "metadata": {},
   "source": [
    "### Regularized logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ec4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_regularized_gradient_descent(y, x, gamma):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86772c4",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "In this section we use the cross validation on the splitted and preprocessed datasets with pre-processing parameters that are common for all the methods. Then we're going to choose the method with the largest accuracy and explore the best pre-processing parameters for that specific model. In our cas probably ridge regression... let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76649612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2e08d9",
   "metadata": {},
   "source": [
    "# Get estimates from Ridge-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "seed = 7\n",
    "\n",
    "# canditates parameters\n",
    "degrees_candidates = [2,2,4,5,6,7,8,9,10]\n",
    "alphas_candidates=[2,3,4,5,6,7,8,9]\n",
    "lambdas_candidates = [1e-02, 1e-03, 1e-06, 1e-07]\n",
    "\n",
    "\n",
    "k_fold = 3\n",
    "\n",
    "opt_degree, opt_lambda, opt_alpha, accuracy = select_parameters_ridge_regression_jet(y,x,degrees_candidates,lambdas_candidates,\n",
    "                                                                  alphas_candidates,k_fold,seed)\n",
    "print('Optimal alphas per jet_class:',opt_alpha)\n",
    "print('Optimal degrees per jet_class:',opt_degree)\n",
    "print('Optimal lambdas per jet_class:',opt_lambda)\n",
    "print('Maximum accuracy predicted per jet_class:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da998ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [3, 3, 6, 6]\n",
    "alphas = [7, 9, 5, 5]\n",
    "lambdas = [0.00021544346900318823, 1e-07, 4.641588833612773e-06, 0.00021544346900318823]\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 3\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, x, ridge_regression, k_indices, k, degrees, alphas, lambdas)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "degrees = [3, 3, 6, 6]\n",
    "alphas = [7, 9, 5, 5]\n",
    "lambdas = [0.00021544346900318823, 1e-07, 4.641588833612773e-06, 0.00021544346900318823]\n",
    "\n",
    "\n",
    "jet_train_class = {\n",
    "    0: x[:, 22] == 0,\n",
    "    1: x[:, 22] == 1,\n",
    "    2: x[:, 22] == 2, \n",
    "    3: x[:, 22] == 3\n",
    "}\n",
    "\n",
    "    \n",
    "jet_test_class = {\n",
    "    0: x_test[:, 22] == 0,\n",
    "    1: x_test[:, 22] == 1,\n",
    "    2: x_test[:, 22] == 2, \n",
    "    3: x_test[:, 22] == 3\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    x_jet = x[jet_train_class[i]]\n",
    "    x_jet_test = x_test[jet_test_class[i]]\n",
    "    y_jet = y[jet_train_class[i]]\n",
    "\n",
    "    # Pre-processing and transformation of the training set and test set\n",
    "    x_jet, x_jet_test = pre_process_data(x_jet, x_jet_test, alphas[i])\n",
    "    x_jet = build_poly(x_jet, degrees[i])\n",
    "    x_jet_test = build_poly(x_jet_test, degrees[i])\n",
    "    \n",
    "    # Train the model through Ridge Regression\n",
    "    best_w, _ = ridge_regression(y_jet, x_jet, lambdas[i])\n",
    "    \n",
    "    # Prediction\n",
    "    pred = transform_binary(best_w, x_jet_test)\n",
    "    ridge_pred[jet_test_class[i]] = pred\n",
    "\n",
    "ridge_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    y_pred = np.c_[ids_test, pred].astype(str)\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    np.savetxt(title + \".csv\", y_pred, fmt=\"%s\", delimiter=\",\")\n",
    "\n",
    "savePredictions(ridge_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3eec7",
   "metadata": {},
   "source": [
    "## Step 4 - Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e218a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error Gradient Descent\n",
    "w_gd, loss_gd = mean_squared_error_gd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_gd)\n",
    "print(\"MSE - GD Loss: \", loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Stochastic Gradient Descent\n",
    "w_sgd, loss_sgd = mean_squared_error_sgd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_sgd)\n",
    "print(\"MSE - SGD Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ca13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Squares\n",
    "w_ls, loss_ls = least_squares(y_tr, x)\n",
    "#w_poly, loss_poly = polynomial_regression_ls(y_tr, x)\n",
    "pred = get_predictions(x_te, w_ls)\n",
    "print(\"MSE - LS Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "# TODO: add split data\n",
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "#w_rr, loss_rr = ridge_regression_demo(x, y, degree, split_ratio, seed)\n",
    "w_rr = ridge_regression(y_tr, x, .0005 )\n",
    "pred_rr = get_predictions(x_te, w_rr)\n",
    "#print(\"MSE - RR Loss: \", loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logictic Regression Gradient Descent\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_logreg, loss_logreg = logistic_regression_gradient_descent(y_tr, x)\n",
    "pred_log = get_predictions(x_te, w_logreg)\n",
    "print(\"RMSE - LogRed Loss: \", loss_logreg)\n",
    "y_tr = np.where(y == \"s\",1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f74c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_log, title=\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression with GD\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_reglog, loss_reglog = logistic_regression_regularized_gradient_descent(y_tr, x, .001)\n",
    "pred_reglog = get_predictions(x_te, w_reglog)\n",
    "# submission : 0.680 dont know why loss is increasing but prediction got better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_reglog, title=\"submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e002c473d40394036330ccbc573ff2124176c3c3fbf7652b00c2ff8632cfa48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
