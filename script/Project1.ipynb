{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "from plots import cross_validation_visualization\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import the datasets\n",
    "\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c63ce9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (250000, 30)  y:  (250000,)\n"
     ]
    }
   ],
   "source": [
    "y = train[1]\n",
    "x_tr = train[2:].T\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11c0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_float = x_tr.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf7610b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_float[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b094bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced=replace_with_median(x_tr_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c01168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999.0 112.406\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_float[2,0],replaced[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "370c2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_VIF=colinearity_check(x_tr_float,4)\n",
    "features_VIF_replaced=colinearity_check(replaced,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f05e05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features x-y' 'VIF value' '4-10' '6.524101144326354' '4-22'\n",
      " '5.1596893608980885' '4-30' '5.778013281848558' '5-6'\n",
      " '19.047669981172188' '5-7' '53844.07915228592' '5-13' '619469.2963445598'\n",
      " '5-23' '8.083860994944821' '5-27' '1530.4175157085713' '5-28'\n",
      " '197248.12895290542' '5-29' '224386.1953107767' '5-30' '4.06157170901049'\n",
      " '6-7' '18.51373741116583' '6-13' '18.890975756598333' '6-23'\n",
      " '5.933071846449745' '6-27' '19.61629470863673' '6-28'\n",
      " '18.864456216699192' '6-29' '18.86493411790321' '6-30'\n",
      " '4.152349206013538' '7-13' '96849.9011364278' '7-23' '8.096334811780176'\n",
      " '7-27' '1530.8949719738653' '7-28' '83179.90808920997' '7-29'\n",
      " '87773.28208721284' '7-30' '4.063609786733423' '10-22'\n",
      " '10.994249005835202' '10-23' '4.709509835921955' '10-30'\n",
      " '29.602520510248585' '13-23' '8.088287488606015' '13-27'\n",
      " '1550.0017316503333' '13-28' '330686.36326640844' '13-29'\n",
      " '409261.75027636247' '13-30' '4.062998350213046' '22-30'\n",
      " '9.182150549610979' '23-24' '6.515615502517554' '23-25'\n",
      " '6.025504079174276' '23-26' '6.025720072438802' '23-27'\n",
      " '8.20479221534431' '23-28' '8.08908774107135' '23-29' '8.088870937543987'\n",
      " '23-30' '5.664754103299299' '24-25' '256.91203953546426' '24-26'\n",
      " '256.8978004137104' '25-26' '122502.87778944003' '27-28'\n",
      " '1544.5386969749165' '27-29' '1543.356431279963' '27-30'\n",
      " '4.268957712045245' '28-29' '191351.4616625482' '28-30'\n",
      " '4.063113903965437' '29-30' '4.062997957027839']\n"
     ]
    }
   ],
   "source": [
    "print(features_VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e704bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features x-y' 'VIF value' '1-3' '5.686226789480798' '4-10'\n",
      " '6.524101144326354' '4-22' '5.1596893608980885' '4-24'\n",
      " '4.840687535901952' '4-30' '5.778013281848558' '5-6' '5.455266031311051'\n",
      " '5-7' '6.758915407980832' '6-7' '4.775765880064227' '10-22'\n",
      " '10.994249005835202' '10-23' '4.709509835921955' '10-24'\n",
      " '5.76077498400385' '10-30' '29.602520510248585' '22-24'\n",
      " '4.247982828249272' '22-30' '9.182150549610979' '23-30'\n",
      " '5.664754103299299' '24-30' '5.637740057190627']\n"
     ]
    }
   ],
   "source": [
    "print(features_VIF_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)  and y[:5]:  [ 1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "y = train[1]\n",
    "y_tr = np.where(y == \"s\",1,-1)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 19), (568238, 19))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94764b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_2 = np.genfromtxt(\"x_tr_2.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_2 = np.array(x_tr_2).T\n",
    "\n",
    "x_te_2 = np.genfromtxt(\"x_te_2.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_2 = np.array(x_te_2).T\n",
    "\n",
    "x_tr_3 = np.genfromtxt(\"x_tr_3_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_3 = np.array(x_tr_3).T\n",
    "\n",
    "x_te_3 = np.genfromtxt(\"x_te_3_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_3 = np.array(x_te_3).T\n",
    "\n",
    "x_tr_4 = np.genfromtxt(\"x_tr_4_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_4 = np.array(x_tr_4).T\n",
    "x_te_4 = np.genfromtxt(\"x_te_4_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_4 = np.array(x_te_4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec703350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 23), (568238, 23))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_4.shape, x_te_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41664a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_4)\n",
    "x_te, _, _ = standardize(x_te_4)\n",
    "#x = x_tr_2\n",
    "#x_te = x_te_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef398d78",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # print w and loss\n",
    "        #print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial w\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        #print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229356",
   "metadata": {},
   "source": [
    "#### Build polimonial basis function which can be used with Least Squares and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a593cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "        \n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x),1))\n",
    "    for j in range( 1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, j)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5430e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_ls(y, x, degrees=[1, 3, 7, 12]):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    ws = []\n",
    "    losses = []\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(x, degree)\n",
    "        w, loss = least_squares(y, tx)\n",
    "        rmse = np.sqrt(2 * loss)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(rmse)\n",
    "\n",
    "        #print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "         #     i=ind + 1, d=degree, loss=rmse))\n",
    "    ind = argmin(losses)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecad3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # TODO split and add test data\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(compute_rmse(y_tr, tx_tr, weight))\n",
    "        \n",
    "        #print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "         #      p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    ind = argmin(rmse_tr)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y âˆˆ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca38bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    max_iter = 1200\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f0bae",
   "metadata": {},
   "source": [
    "#### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47b8018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_regularized_gradient_descent(y, x, gamma):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61f74888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    gamma = 1.\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], )), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3eec7",
   "metadata": {},
   "source": [
    "## Step 4 - Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_w).reshape((x.shape[0],))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fb452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    np.savetxt(title + \".csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61e218a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n",
      "MSE - GD Loss:  0.3769979428714101\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Gradient Descent\n",
    "w_gd, loss_gd = mean_squared_error_gd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_gd)\n",
    "print(\"MSE - GD Loss: \", loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8cbc6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n",
      "MSE - SGD Loss:  0.37709477456844276\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Stochastic Gradient Descent\n",
    "w_sgd, loss_sgd = mean_squared_error_sgd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_sgd)\n",
    "print(\"MSE - SGD Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "639ca13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "MSE - LS Loss:  0.37709477456844276\n"
     ]
    }
   ],
   "source": [
    "# Least Squares\n",
    "w_ls, loss_ls = least_squares(y_tr, x)\n",
    "#w_poly, loss_poly = polynomial_regression_ls(y_tr, x)\n",
    "pred = get_predictions(x_te, w_ls)\n",
    "print(\"MSE - LS Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06cc5f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '1']\n",
      " ['350003' '1']\n",
      " ['350004' '1']]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "# TODO: add split data\n",
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "#w_rr, loss_rr = ridge_regression_demo(x, y, degree, split_ratio, seed)\n",
    "w_rr = ridge_regression(y_tr, x, .0005 )\n",
    "pred_rr = get_predictions(x_te, w_rr)\n",
    "#print(\"MSE - RR Loss: \", loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfa7a9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599171\n",
      "Current iteration=100, loss=0.528434491680455\n",
      "Current iteration=200, loss=0.5279865452731254\n",
      "Current iteration=300, loss=0.5277853934851616\n",
      "Current iteration=400, loss=0.5276226533985445\n",
      "Current iteration=500, loss=0.5274731854011556\n",
      "Current iteration=600, loss=0.5273299754574311\n",
      "Current iteration=700, loss=0.527190778720534\n",
      "Current iteration=800, loss=0.5270548266640629\n",
      "Current iteration=900, loss=0.5269218187425844\n",
      "Current iteration=1000, loss=0.5267916046422771\n",
      "Current iteration=1100, loss=0.5266640837045103\n",
      "loss=0.5265391730037469\n",
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "RMSE - LogRed Loss:  0.5265404094417004\n"
     ]
    }
   ],
   "source": [
    "# Logictic Regression Gradient Descent\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_logreg, loss_logreg = logistic_regression_gradient_descent(y_tr, x)\n",
    "pred_log = get_predictions(x_te, w_logreg)\n",
    "print(\"RMSE - LogRed Loss: \", loss_logreg)\n",
    "y_tr = np.where(y == \"s\",1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6f74c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_log, title=\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae2be4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599171\n",
      "Current iteration=100, loss=0.6593094736796007\n",
      "Current iteration=200, loss=0.6425775866149744\n",
      "Current iteration=300, loss=0.6328790714299811\n",
      "Current iteration=400, loss=0.6262761154929832\n",
      "Current iteration=500, loss=0.6211910561382201\n",
      "Current iteration=600, loss=0.616966971233798\n",
      "Current iteration=700, loss=0.6133147751830225\n",
      "Current iteration=800, loss=0.6100957833801411\n",
      "Current iteration=900, loss=0.6072343668802767\n",
      "Current iteration=1000, loss=0.604682337704415\n",
      "Current iteration=1100, loss=0.6024042837718824\n",
      "Current iteration=1200, loss=0.6003714733402234\n",
      "Current iteration=1300, loss=0.5985592728327229\n",
      "Current iteration=1400, loss=0.5969459989278997\n",
      "Current iteration=1500, loss=0.5955123512386361\n",
      "Current iteration=1600, loss=0.5942410786921569\n",
      "Current iteration=1700, loss=0.5931167415824867\n",
      "Current iteration=1800, loss=0.5921255162544622\n",
      "Current iteration=1900, loss=0.5912550230883142\n",
      "Current iteration=2000, loss=0.5904941712185001\n",
      "Current iteration=2100, loss=0.5898330178512723\n",
      "Current iteration=2200, loss=0.5892626413176649\n",
      "Current iteration=2300, loss=0.5887750271829331\n",
      "Current iteration=2400, loss=0.5883629666208339\n",
      "Current iteration=2500, loss=0.5880199661268574\n",
      "Current iteration=2600, loss=0.5877401675652706\n",
      "Current iteration=2700, loss=0.5875182775267387\n",
      "Current iteration=2800, loss=0.5873495050019879\n",
      "Current iteration=2900, loss=0.5872295064356855\n",
      "Current iteration=3000, loss=0.5871543372997601\n",
      "Current iteration=3100, loss=0.5871204094070278\n",
      "loss=0.5602777199704454\n",
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n"
     ]
    }
   ],
   "source": [
    "# Regularized Logistic Regression with GD\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_reglog, loss_reglog = logistic_regression_regularized_gradient_descent(y_tr, x, .001)\n",
    "pred_reglog = get_predictions(x_te, w_reglog)\n",
    "# submission : 0.680 dont know why loss is increasing but prediction got better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2464f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_reglog, title=\"submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51db59b",
   "metadata": {},
   "source": [
    "## Step - 5 : Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b408eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(method, y, x, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\"\"\"\n",
    "    \n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    \n",
    "    \n",
    "    if method == \"ridge_regression_demo\":\n",
    "        # form data with polynomial degree\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        # ridge regression\n",
    "        best_w = ridge_regression(y_tr, tx_tr, lambda_ridge)\n",
    "        loss_tr = sum(get_predictions_cv(tx_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(tx_te, best_w) == y_te)/len(y_te)\n",
    "        #y_pred = get_predictions_cv(tx_te, best_w)\n",
    "        \n",
    "    if method == \"least_squares\":\n",
    "        best_w, loss_ls = least_squares(y_tr, x_tr)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)\n",
    "        \n",
    "    if method == \"mean_squared_error_gd\":\n",
    "        best_w, loss = mean_squared_error_gd(y_tr, x_tr, max_iters = 150, gamma = gamma)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)\n",
    "        \n",
    "    if method == \"mean_squared_error_sgd\":\n",
    "        best_w, loss = mean_squared_error_sgd(y_tr, x_tr, max_iters=150, gamma = gamma)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)    \n",
    "\n",
    "    if method == \"logistic_regression_gradient_descent\":\n",
    "        best_w, loss_logreg = logistic_regression_gradient_descent(y_tr, x_tr)\n",
    "        loss_tr = sum(get_predictions_cv(x_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(x_te, best_w) == y_te)/len(y_te)           \n",
    "        \n",
    "    if method == \"logistic_regression_regularized_gradient_descent\":\n",
    "        best_w, loss_logreg = logistic_regression_regularized_gradient_descent(y_tr, x_tr, lambda_logistic)\n",
    "        loss_tr = sum(get_predictions_cv(x_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(x_te, best_w) == y_te)/len(y_te)                   \n",
    "        \n",
    "    return loss_tr, loss_te, best_w   #, y_pred, y_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b6d9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(method, y, tx, max_iters, gamma, lambdas_ridge, lambdas_logistic):\n",
    "    \n",
    "    if method in (\"mean_squared_error_gd\", \"mean_squared_error_sgd\", \n",
    "                  \"least_squares\", \"logistic_regression_gradient_descent\"):\n",
    "        seed = 7\n",
    "        k_fold = 10\n",
    "        #split data in k fold \n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0 \n",
    "        lambda_logistic = 0\n",
    "        degree = 0\n",
    "        # cross validation:\n",
    "        for k in range(k_fold):\n",
    "            print(\"k fold = \", k+1 , \"/\", k_fold)\n",
    "            loss_tr, loss_te,_ = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "            losses_tr.append(loss_tr)\n",
    "            losses_te.append(loss_te)\n",
    "            \n",
    "        loss_tr = np.mean(losses_tr)\n",
    "        loss_te = np.mean(losses_te)\n",
    "        \n",
    "        return (loss_tr, loss_te)\n",
    "            \n",
    "        \n",
    "    #if method == \"polynomial_regression_ls\":\n",
    "        #####\n",
    "        \n",
    "    if method == \"ridge_regression_demo\":\n",
    "        seed = 7\n",
    "        degree = 7\n",
    "        k_fold = 10\n",
    "        # split data in k fold\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0\n",
    "        degree = 0\n",
    "        # cross validation\n",
    "        step = 1\n",
    "        for lambda_ridge in lambdas_ridge:\n",
    "            losses_tr_tmp = []\n",
    "            losses_te_tmp = []\n",
    "            print(step, \"/\" , len(lambdas_ridge))\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te,_  = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "                losses_tr_tmp.append(loss_tr)\n",
    "                losses_te_tmp.append(loss_te)\n",
    "                print(\"step : \", step, \"k_fold : \", k,\"/\", k_fold)\n",
    "            losses_tr.append(np.mean(losses_tr_tmp))\n",
    "            losses_te.append(np.mean(losses_te_tmp))\n",
    "            step += 1\n",
    "            \n",
    "\n",
    "        #cross_validation_visualization(lambdas, losses_tr, losses_te)\n",
    "        print(\"losses train = \", losses_tr, \"\\n\\n\", \"losses test = \", losses_te)\n",
    "        return (losses_tr, losses_te)\n",
    "        \n",
    "        \n",
    "    if method == \"logistic_regression_regularized_gradient_descent\":\n",
    "        seed = 7\n",
    "        k_fold = 10\n",
    "        # split data in k fold\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0\n",
    "        degree = 0\n",
    "        # cross validation\n",
    "        step = 1\n",
    "        for lambda_logistic in lambdas_logistic:\n",
    "            losses_tr_tmp = []\n",
    "            losses_te_tmp = []\n",
    "            print(step, \"/\" , len(lambdas_logistic))\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te,_  = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "                losses_tr_tmp.append(loss_tr)\n",
    "                losses_te_tmp.append(loss_te)\n",
    "                print(\"step : \", step, \"k_fold : \", k+1,\"/\", k_fold)\n",
    "            losses_tr.append(np.mean(losses_tr_tmp))\n",
    "            losses_te.append(np.mean(losses_te_tmp))\n",
    "            step += 1\n",
    "        print(\"losses train = \", losses_tr, \"\\n\\n\", \"losses test = \", losses_te)\n",
    "        return (losses_tr, losses_te)\n",
    "        \n",
    "    #if method == \"logistic_regression_newton_method\":\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2726ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tr = np.where(y == \"s\",1,-1)\n",
    "# k_fold_cross_validation(\"ridge_regression_demo\", y_tr, x,  150, .5, (4,2,1,.5), (4,2,1,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e94babe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k fold =  1 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5287225528857348\n",
      "Current iteration=200, loss=0.5282869573218939\n",
      "Current iteration=300, loss=0.5280906687761862\n",
      "Current iteration=400, loss=0.5279308318358086\n",
      "Current iteration=500, loss=0.5277836578944178\n",
      "Current iteration=600, loss=0.5276425306240715\n",
      "Current iteration=700, loss=0.5275053250392723\n",
      "Current iteration=800, loss=0.5273713086086897\n",
      "Current iteration=900, loss=0.5272401914583947\n",
      "Current iteration=1000, loss=0.527111826386247\n",
      "Current iteration=1100, loss=0.5269861136630744\n",
      "loss=0.526862970726974\n",
      "k fold =  2 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.528341276060257\n",
      "Current iteration=200, loss=0.5278846588216265\n",
      "Current iteration=300, loss=0.5276810758612985\n",
      "Current iteration=400, loss=0.5275175925698724\n",
      "Current iteration=500, loss=0.5273679174832407\n",
      "Current iteration=600, loss=0.5272246623028308\n",
      "Current iteration=700, loss=0.5270854598951961\n",
      "Current iteration=800, loss=0.5269495025390015\n",
      "Current iteration=900, loss=0.5268164778654381\n",
      "Current iteration=1000, loss=0.5266862326848575\n",
      "Current iteration=1100, loss=0.5265586663255241\n",
      "loss=0.5264336967092466\n",
      "k fold =  3 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5284986850827467\n",
      "Current iteration=200, loss=0.528044808717164\n",
      "Current iteration=300, loss=0.5278398850788855\n",
      "Current iteration=400, loss=0.5276741842105873\n",
      "Current iteration=500, loss=0.5275220606656076\n",
      "Current iteration=600, loss=0.5273763245109443\n",
      "Current iteration=700, loss=0.5272346759673279\n",
      "Current iteration=800, loss=0.5270963293770177\n",
      "Current iteration=900, loss=0.5269609785634826\n",
      "Current iteration=1000, loss=0.5268284710374419\n",
      "Current iteration=1100, loss=0.526698704978245\n",
      "loss=0.526571596575903\n",
      "k fold =  4 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5284369033071032\n",
      "Current iteration=200, loss=0.5279945530545356\n",
      "Current iteration=300, loss=0.5277959828701372\n",
      "Current iteration=400, loss=0.5276349252143837\n",
      "Current iteration=500, loss=0.5274868496476226\n",
      "Current iteration=600, loss=0.5273449321869769\n",
      "Current iteration=700, loss=0.5272069879276892\n",
      "Current iteration=800, loss=0.5270722669931936\n",
      "Current iteration=900, loss=0.5269404743526255\n",
      "Current iteration=1000, loss=0.5268114610788333\n",
      "Current iteration=1100, loss=0.5266851266539924\n",
      "loss=0.5265613879642629\n",
      "k fold =  5 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5281227896756558\n",
      "Current iteration=200, loss=0.5276772000915058\n",
      "Current iteration=300, loss=0.5274786997497029\n",
      "Current iteration=400, loss=0.5273182504835788\n",
      "Current iteration=500, loss=0.5271709460781254\n",
      "Current iteration=600, loss=0.5270298508669472\n",
      "Current iteration=700, loss=0.5268927408176954\n",
      "Current iteration=800, loss=0.5267588523680402\n",
      "Current iteration=900, loss=0.5266278859502594\n",
      "Current iteration=1000, loss=0.5264996913155827\n",
      "Current iteration=1100, loss=0.5263741677015072\n",
      "loss=0.5262512320716817\n",
      "k fold =  6 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.5283352162504492\n",
      "Current iteration=200, loss=0.5278880720313446\n",
      "Current iteration=300, loss=0.5276878327384982\n",
      "Current iteration=400, loss=0.5275254949614365\n",
      "Current iteration=500, loss=0.5273761957168859\n",
      "Current iteration=600, loss=0.5272330550251226\n",
      "Current iteration=700, loss=0.5270938814341195\n",
      "Current iteration=800, loss=0.52695792699971\n",
      "Current iteration=900, loss=0.5268248991915389\n",
      "Current iteration=1000, loss=0.526694651035176\n",
      "Current iteration=1100, loss=0.526567083498938\n",
      "loss=0.5264421146566138\n",
      "k fold =  7 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5286663061276089\n",
      "Current iteration=200, loss=0.5282156257296134\n",
      "Current iteration=300, loss=0.5280134986709744\n",
      "Current iteration=400, loss=0.5278501832580751\n",
      "Current iteration=500, loss=0.5277002488482977\n",
      "Current iteration=600, loss=0.5275566010287913\n",
      "Current iteration=700, loss=0.5274169717129452\n",
      "Current iteration=800, loss=0.5272805858986398\n",
      "Current iteration=900, loss=0.5271471414837579\n",
      "Current iteration=1000, loss=0.527016488020516\n",
      "Current iteration=1100, loss=0.5268885251115992\n",
      "loss=0.5267631701885446\n",
      "k fold =  8 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.5289114443029397\n",
      "Current iteration=200, loss=0.5284670866728124\n",
      "Current iteration=300, loss=0.5282657552621138\n",
      "Current iteration=400, loss=0.5281018290444207\n",
      "Current iteration=500, loss=0.5279508962960039\n",
      "Current iteration=600, loss=0.5278061856760504\n",
      "Current iteration=700, loss=0.5276655296062533\n",
      "Current iteration=800, loss=0.5275281823113567\n",
      "Current iteration=900, loss=0.5273938482268383\n",
      "Current iteration=1000, loss=0.5272623763140072\n",
      "Current iteration=1100, loss=0.5271336634351619\n",
      "loss=0.5270076237211065\n",
      "k fold =  9 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.5280952612580732\n",
      "Current iteration=200, loss=0.5276416387064168\n",
      "Current iteration=300, loss=0.5274378829638389\n",
      "Current iteration=400, loss=0.527273852980155\n",
      "Current iteration=500, loss=0.5271235465519144\n",
      "Current iteration=600, loss=0.5269796579398537\n",
      "Current iteration=700, loss=0.5268398472585467\n",
      "Current iteration=800, loss=0.5267033147642844\n",
      "Current iteration=900, loss=0.5265697492707012\n",
      "Current iteration=1000, loss=0.5264389964750859\n",
      "Current iteration=1100, loss=0.5263109539028161\n",
      "loss=0.5261855375430915\n",
      "k fold =  10 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.528195690073603\n",
      "Current iteration=200, loss=0.5277443528235821\n",
      "Current iteration=300, loss=0.5275410521160839\n",
      "Current iteration=400, loss=0.5273768633708634\n",
      "Current iteration=500, loss=0.5272261728576308\n",
      "Current iteration=600, loss=0.5270818160892699\n",
      "Current iteration=700, loss=0.526941500277458\n",
      "Current iteration=800, loss=0.5268044418989969\n",
      "Current iteration=900, loss=0.5266703360603574\n",
      "Current iteration=1000, loss=0.52653903151037\n",
      "Current iteration=1100, loss=0.52641042771066\n",
      "loss=0.5262844421513434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7157506666666666, 0.715692)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr = np.where(y == \"s\",1,0)\n",
    "k_fold_cross_validation(\"logistic_regression_gradient_descent\", y_tr, x,  150, .5, (4,2,1,.5), (4,2,1,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df6c0e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6593079129670674\n",
      "Current iteration=200, loss=0.6425804225471089\n",
      "Current iteration=300, loss=0.6328882466612418\n",
      "Current iteration=400, loss=0.6262919581562301\n",
      "Current iteration=500, loss=0.621213263322087\n",
      "Current iteration=600, loss=0.6169950166076983\n",
      "Current iteration=700, loss=0.6133480818787893\n",
      "Current iteration=800, loss=0.6101337963318724\n",
      "Current iteration=900, loss=0.60727658033675\n",
      "Current iteration=1000, loss=0.6047283025189679\n",
      "Current iteration=1100, loss=0.6024536058222888\n",
      "loss=0.5917659204280571\n",
      "step :  1 k_fold :  1 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6593627976307622\n",
      "Current iteration=200, loss=0.6426495306430864\n",
      "Current iteration=300, loss=0.6329556241197868\n",
      "Current iteration=400, loss=0.6263517270086588\n",
      "Current iteration=500, loss=0.6212636966714102\n",
      "Current iteration=600, loss=0.6170360867359035\n",
      "Current iteration=700, loss=0.6133804130385825\n",
      "Current iteration=800, loss=0.610158212458703\n",
      "Current iteration=900, loss=0.6072939142594699\n",
      "Current iteration=1000, loss=0.6047393220222419\n",
      "Current iteration=1100, loss=0.6024589914995436\n",
      "loss=0.5917562250254988\n",
      "step :  1 k_fold :  2 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6593093928400444\n",
      "Current iteration=200, loss=0.6425937349613745\n",
      "Current iteration=300, loss=0.6329146865705502\n",
      "Current iteration=400, loss=0.6263302091490153\n",
      "Current iteration=500, loss=0.6212614001445879\n",
      "Current iteration=600, loss=0.6170511231387735\n",
      "Current iteration=700, loss=0.6134104341512764\n",
      "Current iteration=800, loss=0.6102009033835967\n",
      "Current iteration=900, loss=0.6073471774418671\n",
      "Current iteration=1000, loss=0.6048013291836073\n",
      "Current iteration=1100, loss=0.6025281802282151\n",
      "loss=0.5918535899631658\n",
      "step :  1 k_fold :  3 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6592808586343117\n",
      "Current iteration=200, loss=0.6425384759907312\n",
      "Current iteration=300, loss=0.6328353504617069\n",
      "Current iteration=400, loss=0.626229926227823\n",
      "Current iteration=500, loss=0.6211433783128406\n",
      "Current iteration=600, loss=0.6169183952700786\n",
      "Current iteration=700, loss=0.6132657313560796\n",
      "Current iteration=800, loss=0.610046609434731\n",
      "Current iteration=900, loss=0.6071853299262399\n",
      "Current iteration=1000, loss=0.6046336446273298\n",
      "Current iteration=1100, loss=0.602356089392561\n",
      "loss=0.5916544694342103\n",
      "step :  1 k_fold :  4 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6593587718418333\n",
      "Current iteration=200, loss=0.6426311714744843\n",
      "Current iteration=300, loss=0.6329232988335873\n",
      "Current iteration=400, loss=0.6263075330890233\n",
      "Current iteration=500, loss=0.6212095082039231\n",
      "Current iteration=600, loss=0.6169733288262808\n",
      "Current iteration=700, loss=0.6133101711559972\n",
      "Current iteration=800, loss=0.6100813541875495\n",
      "Current iteration=900, loss=0.6072111703821191\n",
      "Current iteration=1000, loss=0.6046513328875277\n",
      "Current iteration=1100, loss=0.6023663318264731\n",
      "loss=0.5916452162953766\n",
      "step :  1 k_fold :  5 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.6592852003030825\n",
      "Current iteration=200, loss=0.6425428832032622\n",
      "Current iteration=300, loss=0.6328377797372698\n",
      "Current iteration=400, loss=0.6262294900453099\n",
      "Current iteration=500, loss=0.6211397730538264\n",
      "Current iteration=600, loss=0.6169116111843093\n",
      "Current iteration=700, loss=0.613255905559417\n",
      "Current iteration=800, loss=0.6100339502252332\n",
      "Current iteration=900, loss=0.6071700769125661\n",
      "Current iteration=1000, loss=0.6046160471413223\n",
      "Current iteration=1100, loss=0.6023363944699608\n",
      "loss=0.5916286769579117\n",
      "step :  1 k_fold :  6 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6593943662815633\n",
      "Current iteration=200, loss=0.6427037193995782\n",
      "Current iteration=300, loss=0.6330286803898667\n",
      "Current iteration=400, loss=0.62644105908561\n",
      "Current iteration=500, loss=0.6213669707197126\n",
      "Current iteration=600, loss=0.6171511198125238\n",
      "Current iteration=700, loss=0.6135052152639475\n",
      "Current iteration=800, loss=0.6102910245675958\n",
      "Current iteration=900, loss=0.6074332178159434\n",
      "Current iteration=1000, loss=0.604883829044585\n",
      "Current iteration=1100, loss=0.6026076230951218\n",
      "loss=0.5919276026711712\n",
      "step :  1 k_fold :  7 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.659469338430769\n",
      "Current iteration=200, loss=0.6428042298987207\n",
      "Current iteration=300, loss=0.6331355079708295\n",
      "Current iteration=400, loss=0.6265472012225606\n",
      "Current iteration=500, loss=0.6214704129683929\n",
      "Current iteration=600, loss=0.6172518563585664\n",
      "Current iteration=700, loss=0.6136039783862628\n",
      "Current iteration=800, loss=0.6103887372926585\n",
      "Current iteration=900, loss=0.6075307654046089\n",
      "Current iteration=1000, loss=0.6049819735927606\n",
      "Current iteration=1100, loss=0.6027069817865836\n",
      "loss=0.5920304839581658\n",
      "step :  1 k_fold :  8 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.6591574869835819\n",
      "Current iteration=200, loss=0.6423610173781971\n",
      "Current iteration=300, loss=0.6326350497210499\n",
      "Current iteration=400, loss=0.6260206680342058\n",
      "Current iteration=500, loss=0.620931156932344\n",
      "Current iteration=600, loss=0.6167056112993904\n",
      "Current iteration=700, loss=0.6130531715314802\n",
      "Current iteration=800, loss=0.6098343666951472\n",
      "Current iteration=900, loss=0.6069732282403977\n",
      "Current iteration=1000, loss=0.6044214301515478\n",
      "Current iteration=1100, loss=0.6021435118039118\n",
      "loss=0.5914301537592691\n",
      "step :  1 k_fold :  9 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6591673520685719\n",
      "Current iteration=200, loss=0.64236867198465\n",
      "Current iteration=300, loss=0.6326339537164402\n",
      "Current iteration=400, loss=0.6260084369059706\n",
      "Current iteration=500, loss=0.6209077180205218\n",
      "Current iteration=600, loss=0.6166719932200057\n",
      "Current iteration=700, loss=0.6130108298933524\n",
      "Current iteration=800, loss=0.6097848388520996\n",
      "Current iteration=900, loss=0.6069179705241644\n",
      "Current iteration=1000, loss=0.6043617509183611\n",
      "Current iteration=1100, loss=0.6020805524439468\n",
      "loss=0.5913515442570987\n",
      "step :  1 k_fold :  10 / 10\n",
      "2 / 3\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6120559599325581\n",
      "Current iteration=200, loss=0.6415209014326538\n",
      "Current iteration=300, loss=0.6591107467967803\n",
      "Current iteration=400, loss=0.6705735674121347\n",
      "Current iteration=500, loss=0.6782979720227899\n",
      "Current iteration=600, loss=0.6836548514254873\n",
      "Current iteration=700, loss=0.6875146174990674\n",
      "Current iteration=800, loss=0.6904294343285258\n",
      "Current iteration=900, loss=0.6927465790341231\n",
      "Current iteration=1000, loss=0.6946843471113241\n",
      "Current iteration=1100, loss=0.6963809095746678\n",
      "loss=0.5282006418809868\n",
      "step :  2 k_fold :  1 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6121130225770345\n",
      "Current iteration=200, loss=0.6418683415191235\n",
      "Current iteration=300, loss=0.6597302836064897\n",
      "Current iteration=400, loss=0.6714170915160407\n",
      "Current iteration=500, loss=0.6793235996138305\n",
      "Current iteration=600, loss=0.6848313909078216\n",
      "Current iteration=700, loss=0.6888196604126307\n",
      "Current iteration=800, loss=0.691846738472937\n",
      "Current iteration=900, loss=0.6942639973773629\n",
      "Current iteration=1000, loss=0.6962924139518192\n",
      "Current iteration=1100, loss=0.698071935920069\n",
      "loss=0.5277946887756534\n",
      "step :  2 k_fold :  2 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6121638572763496\n",
      "Current iteration=200, loss=0.6419396076073132\n",
      "Current iteration=300, loss=0.6598362648243614\n",
      "Current iteration=400, loss=0.6715360408978567\n",
      "Current iteration=500, loss=0.6794368177669086\n",
      "Current iteration=600, loss=0.6849289012765631\n",
      "Current iteration=700, loss=0.68889793723757\n",
      "Current iteration=800, loss=0.6919060216352527\n",
      "Current iteration=900, loss=0.6943064588303003\n",
      "Current iteration=1000, loss=0.6963210802002626\n",
      "Current iteration=1100, loss=0.6980901037229444\n",
      "loss=0.5279544890513085\n",
      "step :  2 k_fold :  3 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6119886737645557\n",
      "Current iteration=200, loss=0.6416090417286643\n",
      "Current iteration=300, loss=0.6593277881046034\n",
      "Current iteration=400, loss=0.6708843253376109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=500, loss=0.6786782560626828\n",
      "Current iteration=600, loss=0.6840893373345395\n",
      "Current iteration=700, loss=0.687993495014426\n",
      "Current iteration=800, loss=0.6909461331394032\n",
      "Current iteration=900, loss=0.6932964293459811\n",
      "Current iteration=1000, loss=0.6952638253585399\n",
      "Current iteration=1100, loss=0.6969872186464074\n",
      "loss=0.5279071015343855\n",
      "step :  2 k_fold :  4 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6120347359008002\n",
      "Current iteration=200, loss=0.6418600403732531\n",
      "Current iteration=300, loss=0.6597670028766635\n",
      "Current iteration=400, loss=0.6714643348816662\n",
      "Current iteration=500, loss=0.6793562546395726\n",
      "Current iteration=600, loss=0.6848344211944043\n",
      "Current iteration=700, loss=0.688784916265134\n",
      "Current iteration=800, loss=0.6917702123254748\n",
      "Current iteration=900, loss=0.694144066645025\n",
      "Current iteration=1000, loss=0.6961288138816203\n",
      "Current iteration=1100, loss=0.6978651734325919\n",
      "loss=0.5275896652257197\n",
      "step :  2 k_fold :  5 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.611971412411903\n",
      "Current iteration=200, loss=0.64164319086796\n",
      "Current iteration=300, loss=0.6594194143410734\n",
      "Current iteration=400, loss=0.6710276851628156\n",
      "Current iteration=500, loss=0.6788666749263729\n",
      "Current iteration=600, loss=0.68431650868395\n",
      "Current iteration=700, loss=0.6882537914411003\n",
      "Current iteration=800, loss=0.6912347395905751\n",
      "Current iteration=900, loss=0.6936093590204455\n",
      "Current iteration=1000, loss=0.6955978586491314\n",
      "Current iteration=1100, loss=0.697339805280428\n",
      "loss=0.527799852459089\n",
      "step :  2 k_fold :  6 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6122439692057732\n",
      "Current iteration=200, loss=0.6418646278368962\n",
      "Current iteration=300, loss=0.6595779359245169\n",
      "Current iteration=400, loss=0.6711442315324712\n",
      "Current iteration=500, loss=0.6789606076863924\n",
      "Current iteration=600, loss=0.6844016932070149\n",
      "Current iteration=700, loss=0.6883394907972259\n",
      "Current iteration=800, loss=0.6913269391795578\n",
      "Current iteration=900, loss=0.6937118166380623\n",
      "Current iteration=1000, loss=0.6957128414333347\n",
      "Current iteration=1100, loss=0.6974685875635731\n",
      "loss=0.528126488211639\n",
      "step :  2 k_fold :  7 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6123147150238513\n",
      "Current iteration=200, loss=0.6418782405210364\n",
      "Current iteration=300, loss=0.6595529428294936\n",
      "Current iteration=400, loss=0.671072133633421\n",
      "Current iteration=500, loss=0.6788377660466268\n",
      "Current iteration=600, loss=0.6842301682656815\n",
      "Current iteration=700, loss=0.6881239895199892\n",
      "Current iteration=800, loss=0.6910728133468169\n",
      "Current iteration=900, loss=0.6934241698986884\n",
      "Current iteration=1000, loss=0.6953962462531991\n",
      "Current iteration=1100, loss=0.6971270683065992\n",
      "loss=0.5283785496418367\n",
      "step :  2 k_fold :  8 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.6117968958599685\n",
      "Current iteration=200, loss=0.6415043954346432\n",
      "Current iteration=300, loss=0.6592867616207634\n",
      "Current iteration=400, loss=0.6708944984179623\n",
      "Current iteration=500, loss=0.6787345599316128\n",
      "Current iteration=600, loss=0.6841902338299515\n",
      "Current iteration=700, loss=0.6881389360898744\n",
      "Current iteration=800, loss=0.6911364568852084\n",
      "Current iteration=900, loss=0.6935318169750295\n",
      "Current iteration=1000, loss=0.6955440927250202\n",
      "Current iteration=1100, loss=0.6973117894069513\n",
      "loss=0.527551688204709\n",
      "step :  2 k_fold :  9 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6117112299582623\n",
      "Current iteration=200, loss=0.6412319275082283\n",
      "Current iteration=300, loss=0.6588556073945322\n",
      "Current iteration=400, loss=0.6703531596852139\n",
      "Current iteration=500, loss=0.6781213092539241\n",
      "Current iteration=600, loss=0.6835305499335842\n",
      "Current iteration=700, loss=0.6874486804805199\n",
      "Current iteration=800, loss=0.6904252069607953\n",
      "Current iteration=900, loss=0.6928053029805965\n",
      "Current iteration=1000, loss=0.6948057844341661\n",
      "Current iteration=1100, loss=0.6965638712635343\n",
      "loss=0.5276546933271936\n",
      "step :  2 k_fold :  10 / 10\n",
      "3 / 3\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6785867194691976\n",
      "Current iteration=200, loss=0.6947406748883044\n",
      "Current iteration=300, loss=0.7021403314219239\n",
      "Current iteration=400, loss=0.7089507195017555\n",
      "Current iteration=500, loss=0.7163585981054693\n",
      "Current iteration=600, loss=0.7246899935851137\n",
      "Current iteration=700, loss=0.7340798062122829\n",
      "Current iteration=800, loss=0.744585675770916\n",
      "Current iteration=900, loss=0.756224137823867\n",
      "Current iteration=1000, loss=0.7689885992935879\n",
      "Current iteration=1100, loss=0.7828596332104025\n",
      "loss=0.526862970726974\n",
      "step :  3 k_fold :  1 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6796176965249114\n",
      "Current iteration=200, loss=0.696350695291289\n",
      "Current iteration=300, loss=0.7041075831722206\n",
      "Current iteration=400, loss=0.711174199490258\n",
      "Current iteration=500, loss=0.71878187194758\n",
      "Current iteration=600, loss=0.7272856462608703\n",
      "Current iteration=700, loss=0.7368391458424874\n",
      "Current iteration=800, loss=0.7475113585414161\n",
      "Current iteration=900, loss=0.7593253684967655\n",
      "Current iteration=1000, loss=0.7722781938691188\n",
      "Current iteration=1100, loss=0.7863522806192069\n",
      "loss=0.5264336967092466\n",
      "step :  3 k_fold :  2 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6797317287950524\n",
      "Current iteration=200, loss=0.6963787731464319\n",
      "Current iteration=300, loss=0.704114237512578\n",
      "Current iteration=400, loss=0.7112227767560032\n",
      "Current iteration=500, loss=0.7189181534539786\n",
      "Current iteration=600, loss=0.7275458321213085\n",
      "Current iteration=700, loss=0.7372535789408322\n",
      "Current iteration=800, loss=0.748106426556492\n",
      "Current iteration=900, loss=0.7601246587334649\n",
      "Current iteration=1000, loss=0.7733032330365764\n",
      "Current iteration=1100, loss=0.7876230104823251\n",
      "loss=0.526571596575903\n",
      "step :  3 k_fold :  3 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6789692304965959\n",
      "Current iteration=200, loss=0.69532077868213\n",
      "Current iteration=300, loss=0.7028344487854913\n",
      "Current iteration=400, loss=0.709726012988527\n",
      "Current iteration=500, loss=0.7171993324970454\n",
      "Current iteration=600, loss=0.7255899939680202\n",
      "Current iteration=700, loss=0.7350383137813628\n",
      "Current iteration=800, loss=0.74560484902053\n",
      "Current iteration=900, loss=0.7573075865666352\n",
      "Current iteration=1000, loss=0.7701405530519049\n",
      "Current iteration=1100, loss=0.7840844824031461\n",
      "loss=0.5265613879642629\n",
      "step :  3 k_fold :  4 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6796513012441345\n",
      "Current iteration=200, loss=0.6961867815806442\n",
      "Current iteration=300, loss=0.7037387981930974\n",
      "Current iteration=400, loss=0.7106306628811525\n",
      "Current iteration=500, loss=0.7180831986450309\n",
      "Current iteration=600, loss=0.7264376834697165\n",
      "Current iteration=700, loss=0.7358373754219294\n",
      "Current iteration=800, loss=0.7463446206467725\n",
      "Current iteration=900, loss=0.7579785527729355\n",
      "Current iteration=1000, loss=0.7707339479215702\n",
      "Current iteration=1100, loss=0.7845920389187855\n",
      "loss=0.5262512320716817\n",
      "step :  3 k_fold :  5 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.6791589021831866\n",
      "Current iteration=200, loss=0.6956553953138868\n",
      "Current iteration=300, loss=0.703245823057481\n",
      "Current iteration=400, loss=0.7101967604505963\n",
      "Current iteration=500, loss=0.7177304501087336\n",
      "Current iteration=600, loss=0.7261893196039648\n",
      "Current iteration=700, loss=0.7357165026556262\n",
      "Current iteration=800, loss=0.7463737515660228\n",
      "Current iteration=900, loss=0.7581795319957644\n",
      "Current iteration=1000, loss=0.7711280223856034\n",
      "Current iteration=1100, loss=0.7851999582260076\n",
      "loss=0.5264421146566138\n",
      "step :  3 k_fold :  6 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6792511886422464\n",
      "Current iteration=200, loss=0.6957703085566288\n",
      "Current iteration=300, loss=0.703432572258096\n",
      "Current iteration=400, loss=0.7104476499437101\n",
      "Current iteration=500, loss=0.7180340313784705\n",
      "Current iteration=600, loss=0.7265379434308347\n",
      "Current iteration=700, loss=0.7361064058124539\n",
      "Current iteration=800, loss=0.7468039524863249\n",
      "Current iteration=900, loss=0.7586508193293546\n",
      "Current iteration=1000, loss=0.7716422388874192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1100, loss=0.7857595379693713\n",
      "loss=0.5267631701885446\n",
      "step :  3 k_fold :  7 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6791274123285926\n",
      "Current iteration=200, loss=0.695452732699678\n",
      "Current iteration=300, loss=0.7030226751932038\n",
      "Current iteration=400, loss=0.7100068641589818\n",
      "Current iteration=500, loss=0.7176043345448865\n",
      "Current iteration=600, loss=0.7261484638185951\n",
      "Current iteration=700, loss=0.735777258427349\n",
      "Current iteration=800, loss=0.7465491812124759\n",
      "Current iteration=900, loss=0.7584804234805027\n",
      "Current iteration=1000, loss=0.7715634643766436\n",
      "Current iteration=1100, loss=0.7857776728622987\n",
      "loss=0.5270076237211065\n",
      "step :  3 k_fold :  8 / 10\n",
      "Current iteration=0, loss=0.6931471805599201\n",
      "Current iteration=100, loss=0.6790260992592215\n",
      "Current iteration=200, loss=0.6956015317015358\n",
      "Current iteration=300, loss=0.703325884162671\n",
      "Current iteration=400, loss=0.7103954084754825\n",
      "Current iteration=500, loss=0.7180226583021709\n",
      "Current iteration=600, loss=0.7265571307196397\n",
      "Current iteration=700, loss=0.736149649834848\n",
      "Current iteration=800, loss=0.7468673728612989\n",
      "Current iteration=900, loss=0.7587320856082171\n",
      "Current iteration=1000, loss=0.7717398502484758\n",
      "Current iteration=1100, loss=0.7858723845517324\n",
      "loss=0.5261855375430915\n",
      "step :  3 k_fold :  9 / 10\n",
      "Current iteration=0, loss=0.69314718055992\n",
      "Current iteration=100, loss=0.6784091452800034\n",
      "Current iteration=200, loss=0.6948626389723765\n",
      "Current iteration=300, loss=0.7025510749190569\n",
      "Current iteration=400, loss=0.7096048596108114\n",
      "Current iteration=500, loss=0.7172326324872651\n",
      "Current iteration=600, loss=0.7257809024301612\n",
      "Current iteration=700, loss=0.7353977998153174\n",
      "Current iteration=800, loss=0.7461486595807587\n",
      "Current iteration=900, loss=0.7580541861880905\n",
      "Current iteration=1000, loss=0.771109844864977\n",
      "Current iteration=1100, loss=0.7852970417256466\n",
      "loss=0.5262844421513434\n",
      "step :  3 k_fold :  10 / 10\n",
      "losses train =  [0.657332, 0.7154617777777778, 0.7157506666666666] \n",
      "\n",
      " losses test =  [0.657332, 0.715428, 0.715692]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.657332, 0.7154617777777778, 0.7157506666666666],\n",
       " [0.657332, 0.715428, 0.715692])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_cross_validation(\"logistic_regression_regularized_gradient_descent\", \n",
    "                        y_tr, x,  150, .5, (4,2,1,.5), (0.001, 0.1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c18eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e002c473d40394036330ccbc573ff2124176c3c3fbf7652b00c2ff8632cfa48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
