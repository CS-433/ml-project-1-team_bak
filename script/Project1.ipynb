{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f856dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 30), (250000,), (568238, 30))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "import csv\n",
    "def readCSVToNumpyArray(dataset):\n",
    "    values = [[]]\n",
    "    with open(dataset) as f:\n",
    "        counter = 0\n",
    "        for i in csv.reader(f):\n",
    "            for j in i:\n",
    "                try:\n",
    "                    values[counter].append(float(j))\n",
    "                except ValueError:\n",
    "                    values[counter].append(j)\n",
    "            counter = counter + 1\n",
    "            values.append([])\n",
    "\n",
    "    data = np.array(values[:-1],dtype='object')\n",
    "\n",
    "    return data\n",
    "\n",
    "train = readCSVToNumpyArray('train.csv')[1:,]\n",
    "test = readCSVToNumpyArray('test.csv')[1:,]\n",
    "x_tr = train[:,2:]\n",
    "y_tr = train[:,1]\n",
    "x_te = test[:,2:]\n",
    "\n",
    "train[:,0] = [int(x) for x in train[:,0]]\n",
    "test[:,0] = [int(x) for x in test[:,0]]\n",
    "\n",
    "x_tr.shape, y_tr.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "#train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "#train = np.array(train_list)\n",
    "#test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "#test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "452c81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_tr = train[1]\n",
    "#x_tr = train[2:].T\n",
    "#print(\"x: \", x_tr.shape, \" y: \", y_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)  and y[:5]:  [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_tr = np.where(y_tr == \"s\",1,0)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 18)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 19), (568238, 19))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9135d0",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_w)\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[:, 0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0669472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61e218a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/149: loss=0.171334, w0=0.0017133399999999998, w1=0.00043645910164310674\n",
      "GD iter. 1/149: loss=0.1649925688549367, w0=0.00332096214587879, w1=0.0008386390404597022\n",
      "GD iter. 2/149: loss=0.1596486017372754, w0=0.0048317425994134165, w1=0.00120943363357687\n",
      "GD iter. 3/149: loss=0.15513735265013423, w0=0.006253789666325029, w1=0.0015514862427284718\n",
      "GD iter. 4/149: loss=0.15132140940056046, w0=0.007594510397486284, w1=0.0018672114987907896\n",
      "GD iter. 5/149: loss=0.14808615665186411, w0=0.008860671412039476, w1=0.002158815141566262\n",
      "GD iter. 6/149: loss=0.14533599206028733, w0=0.010058454443722461, w1=0.0024283121383331563\n",
      "GD iter. 7/149: loss=0.14299117049171767, w0=0.0111935070682075, w1=0.0026775432304924586\n",
      "GD iter. 8/149: loss=0.14098517206359573, w0=0.012270989029539839, w1=0.0029081900446875342\n",
      "GD iter. 9/149: loss=0.139262507062407, w0=0.013295614547490212, w1=0.003121788892940387\n",
      "GD iter. 10/149: loss=0.13777688521990067, w0=0.014271690954509957, w1=0.0033197433755431014\n",
      "GD iter. 11/149: loss=0.13648968886823354, w0=0.015203153980725682, w1=0.0035033358905752978\n",
      "GD iter. 12/149: loss=0.13536869953327266, w0=0.016093599977783354, w1=0.0036737381449067495\n",
      "GD iter. 13/149: loss=0.13438703589793838, w0=0.016946315347121536, w1=0.0038320207533144865\n",
      "GD iter. 14/149: loss=0.13352226805035272, w0=0.01776430341521215, w1=0.003979162004827888\n",
      "GD iter. 15/149: loss=0.1327556787553462, w0=0.018550308977264972, w1=0.0041160558685515046\n",
      "GD iter. 16/149: loss=0.132071647344979, w0=0.019306840711675367, w1=0.004243519304947084\n",
      "GD iter. 17/149: loss=0.13145713587460534, w0=0.020036191649945332, w1=0.004362298942831824\n",
      "GD iter. 18/149: loss=0.13090126056947599, w0=0.020740457870781093, w1=0.0044730771771220625\n",
      "GD iter. 19/149: loss=0.1303949344045547, w0=0.021421555572433946, w1=0.004576477737577401\n",
      "GD iter. 20/149: loss=0.12993056901019176, w0=0.022081236663984632, w1=0.004673070774440177\n",
      "GD iter. 21/149: loss=0.12950182605620164, w0=0.022721103004064446, w1=0.004763377502883455\n",
      "GD iter. 22/149: loss=0.1291034099014662, w0=0.02334261940435853, w1=0.004847874444544393\n",
      "GD iter. 23/149: loss=0.1287308946594386, w0=0.023947125505056052, w1=0.004926997301098966\n",
      "GD iter. 24/149: loss=0.12838057996689273, w0=0.02453584662011453, w1=0.005001144491801366\n",
      "GD iter. 25/149: loss=0.12804937069150146, w0=0.025109903641714792, w1=0.005070680384141718\n",
      "GD iter. 26/149: loss=0.12773467660467505, w0=0.02567032208552889, w1=0.005135938244246468\n",
      "GD iter. 27/149: loss=0.1274343287056592, w0=0.02621804035134185, w1=0.005197222931335919\n",
      "GD iter. 28/149: loss=0.12714650943298494, w0=0.026753917267101142, w1=0.005254813358443861\n",
      "GD iter. 29/149: loss=0.1268696944581402, w0=0.027278738978561704, w1=0.005308964739677848\n",
      "GD iter. 30/149: loss=0.12660260413896254, w0=0.027793225241300873, w1=0.00535991064253926\n",
      "GD iter. 31/149: loss=0.12634416302936607, w0=0.02829803516695185, w1=0.00540786486221568\n",
      "GD iter. 32/149: loss=0.1260934661081624, w0=0.028793772471006056, w1=0.0054530231332907365\n",
      "GD iter. 33/149: loss=0.12584975061170248, w0=0.0292809902654267, w1=0.005495564692976635\n",
      "GD iter. 34/149: loss=0.1256123725401891, w0=0.029760195435564265, w1=0.005535653708750803\n",
      "GD iter. 35/149: loss=0.1253807870619069, w0=0.030231852637438432, w1=0.005573440582160536\n",
      "GD iter. 36/149: loss=0.12515453216838202, w0=0.030696387948322097, w1=0.005609063139538902\n",
      "GD iter. 37/149: loss=0.12493321504087529, w0=0.031154192200705713, w1=0.005642647719443102\n",
      "GD iter. 38/149: loss=0.12471650067818367, w0=0.031605624027110604, w1=0.005674310165775274\n",
      "GD iter. 39/149: loss=0.1245041024104195, w0=0.03205101264083682, w1=0.005704156734768388\n",
      "GD iter. 40/149: loss=0.12429577398574113, w0=0.03249066037555465, w1=0.005732284923309953\n",
      "GD iter. 41/149: loss=0.12409130296896538, w0=0.032924845004661495, w1=0.005758784225427945\n",
      "GD iter. 42/149: loss=0.12389050523432776, w0=0.033353821859510496, w1=0.005783736823171295\n",
      "GD iter. 43/149: loss=0.12369322037079712, w0=0.03377782576395981, w1=0.005807218217576558\n",
      "GD iter. 44/149: loss=0.12349930784849382, w0=0.03419707280117754, w1=0.0058292978049185986\n",
      "GD iter. 45/149: loss=0.12330864381989987, w0=0.03461176192725488, w1=0.005850039402992178\n",
      "GD iter. 46/149: loss=0.12312111845051465, w0=0.035022076444917345, w1=0.00586950173175948\n",
      "GD iter. 47/149: loss=0.12293663369109727, w0=0.03542818534947119, w1=0.005887738852322532\n",
      "GD iter. 48/149: loss=0.12275510141821816, w0=0.03583024455806892, w1=0.005904800567835993\n",
      "GD iter. 49/149: loss=0.1225764418820073, w0=0.03622839803241626, w1=0.005920732789662121\n",
      "GD iter. 50/149: loss=0.12240058241012917, w0=0.036622778804164764, w1=0.005935577871783264\n",
      "GD iter. 51/149: loss=0.12222745632547463, w0=0.03701350991143223, w1=0.005949374916225607\n",
      "GD iter. 52/149: loss=0.12205700204211613, w0=0.03740070525416066, w1=0.005962160052009031\n",
      "GD iter. 53/149: loss=0.12188916230995768, w0=0.0377844703753526, w1=0.005973966689919709\n",
      "GD iter. 54/149: loss=0.12172388358341757, w0=0.03816490317461591, w1=0.005984825755202861\n",
      "GD iter. 55/149: loss=0.12156111549357644, w0=0.038542094559889106, w1=0.005994765900091087\n",
      "GD iter. 56/149: loss=0.12140081040663615, w0=0.03891612904270992, w1=0.006003813697917537\n",
      "GD iter. 57/149: loss=0.1212429230543827, w0=0.03928708528192467, w1=0.0060119938204114196\n",
      "GD iter. 58/149: loss=0.12108741022471994, w0=0.03965503658031083, w1=0.006019329199634726\n",
      "GD iter. 59/149: loss=0.12093423050232278, w0=0.04002005133819746, w1=0.0060258411758925285\n",
      "GD iter. 60/149: loss=0.12078334405110856, w0=0.040382193467813576, w1=0.00603154963283357\n",
      "GD iter. 61/149: loss=0.12063471243160445, w0=0.040741522771771115, w1=0.006036473120852326\n",
      "GD iter. 62/149: loss=0.12048829844743604, w0=0.041098095288793345, w1=0.006040628969807322\n",
      "GD iter. 63/149: loss=0.12034406601612126, w0=0.04145196360953002, w1=0.00604403339198244\n",
      "GD iter. 64/149: loss=0.12020198006015351, w0=0.04180317716505372, w1=0.006046701576137542\n",
      "GD iter. 65/149: loss=0.12006200641502286, w0=0.042151782490407026, w1=0.006048647773421329\n",
      "GD iter. 66/149: loss=0.11992411175138092, w0=0.042497823465364425, w1=0.006049885375852286\n",
      "GD iter. 67/149: loss=0.11978826350901944, w0=0.04284134153438514, w1=0.00605042698801234\n",
      "GD iter. 68/149: loss=0.1196544298407169, w0=0.043182375907561694, w1=0.006050284492541895\n",
      "GD iter. 69/149: loss=0.11952257956433288, w0=0.04352096374421238, w1=0.006049469109973905\n",
      "GD iter. 70/149: loss=0.11939268212179598, w0=0.043857140320622844, w1=0.006047991453397929\n",
      "GD iter. 71/149: loss=0.11926470754385825, w0=0.044190939183311344, w1=0.006045861578402564\n",
      "GD iter. 72/149: loss=0.11913862641967325, w0=0.044522392289073155, w1=0.006043089028705739\n",
      "GD iter. 73/149: loss=0.1190144098704136, w0=0.04485153013295046, w1=0.006039682877846827\n",
      "GD iter. 74/149: loss=0.11889202952627181, w0=0.045178381865174776, w1=0.006035651767282086\n",
      "GD iter. 75/149: loss=0.11877145750629858, w0=0.04550297539803812, w1=0.006031003941195326\n",
      "GD iter. 76/149: loss=0.11865266640062129, w0=0.04582533750356597, w1=0.006025747278308613\n",
      "GD iter. 77/149: loss=0.11853562925466321, w0=0.04614549390278967, w1=0.006019889320953154\n",
      "GD iter. 78/149: loss=0.1184203195550448, w0=0.04646346934734645, w1=0.006013437301637893\n",
      "GD iter. 79/149: loss=0.11830671121690223, w0=0.046779287694072115, w1=0.006006398167332775\n",
      "GD iter. 80/149: loss=0.11819477857240161, w0=0.04709297197319391, w1=0.005998778601664795\n",
      "GD iter. 81/149: loss=0.11808449636026476, w0=0.04740454445067819, w1=0.005990585045207777\n",
      "GD iter. 82/149: loss=0.11797583971615064, w0=0.04771402668523948, w1=0.00598182371403111\n",
      "GD iter. 83/149: loss=0.11786878416376571, w0=0.048021439580473606, w1=0.005972500616658353\n",
      "GD iter. 84/149: loss=0.11776330560659268, w0=0.04832680343253733, w1=0.0059626215695735224\n",
      "GD iter. 85/149: loss=0.11765938032015061, w0=0.04863013797376041, w1=0.005952192211400915\n",
      "GD iter. 86/149: loss=0.11755698494470862, w0=0.048931462412542454, w1=0.0059412180158734135\n",
      "GD iter. 87/149: loss=0.11745609647839175, w0=0.04923079546985628, w1=0.005929704303694234\n",
      "GD iter. 88/149: loss=0.11735669227062517, w0=0.049528155412651854, w1=0.005917656253387986\n",
      "GD iter. 89/149: loss=0.11725875001587348, w0=0.04982356008442902, w1=0.005905078911228594\n",
      "GD iter. 90/149: loss=0.11716224774763771, w0=0.05011702693322431, w1=0.0058919772003240166\n",
      "GD iter. 91/149: loss=0.11706716383267884, w0=0.05040857303723554, w1=0.005878355928930807\n",
      "GD iter. 92/149: loss=0.11697347696544197, w0=0.050698215128288746, w1=0.005864219798065176\n",
      "GD iter. 93/149: loss=0.11688116616265913, w0=0.05098596961333412, w1=0.005849573408471461\n",
      "GD iter. 94/149: loss=0.11679021075811238, w0=0.051271852594141416, w1=0.005834421267003632\n",
      "GD iter. 95/149: loss=0.1167005903975417, w0=0.05155587988535065, w1=0.005818767792470605\n",
      "GD iter. 96/149: loss=0.11661228503368423, w0=0.05183806703102022, w1=0.005802617320991762\n",
      "GD iter. 97/149: loss=0.11652527492143405, w0=0.05211842931980233, w1=0.005785974110905049\n",
      "GD iter. 98/149: loss=0.11643954061311285, w0=0.052396981798864414, w1=0.005768842347266316\n",
      "GD iter. 99/149: loss=0.11635506295384374, w0=0.052673739286664786, w1=0.005751226145975256\n",
      "GD iter. 100/149: loss=0.11627182307702116, w0=0.05294871638468153, w1=0.005733129557560203\n",
      "GD iter. 101/149: loss=0.11618980239987076, w0=0.05322192748818493, w1=0.005714556570651256\n",
      "GD iter. 102/149: loss=0.11610898261909494, w0=0.053493386796136, w1=0.005695511115168648\n",
      "GD iter. 103/149: loss=0.11602934570659866, w0=0.05376310832028644, w1=0.005675997065250938\n",
      "GD iter. 104/149: loss=0.115950873905293, w0=0.05403110589354886, w1=0.005656018241945472\n",
      "GD iter. 105/149: loss=0.1158735497249718, w0=0.05429739317770016, w1=0.005635578415681626\n",
      "GD iter. 106/149: loss=0.11579735593825974, w0=0.054561983670475406, w1=0.0056146813085455405\n",
      "GD iter. 107/149: loss=0.11572227557662837, w0=0.05482489071210474, w1=0.005593330596373455\n",
      "GD iter. 108/149: loss=0.1156482919264784, w0=0.055086127491341064, w1=0.005571529910679258\n",
      "GD iter. 109/149: loss=0.11557538852528584, w0=0.055345707051022416, w1=0.005549282840430525\n",
      "GD iter. 110/149: loss=0.11550354915781064, w0=0.0556036422932088, w1=0.005526592933686051\n",
      "GD iter. 111/149: loss=0.11543275785236541, w0=0.055859945983930084, w1=0.005503463699106801\n",
      "GD iter. 112/149: loss=0.11536299887714385, w0=0.05611463075757822, w1=0.005479898607351128\n",
      "GD iter. 113/149: loss=0.11529425673660583, w0=0.05636770912097424, w1=0.005455901092364182\n",
      "GD iter. 114/149: loss=0.11522651616792012, w0=0.056619193457137826, w1=0.00543147455257059\n",
      "GD iter. 115/149: loss=0.11515976213746121, w0=0.05686909602878482, w1=0.005406622351978672\n",
      "GD iter. 116/149: loss=0.11509397983736047, w0=0.05711742898157591, w1=0.005381347821203752\n",
      "GD iter. 117/149: loss=0.11502915468211089, w0=0.057364204347137585, w1=0.005355654258417488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 118/149: loss=0.11496527230522288, w0=0.05760943404587481, w1=0.0053295449302295135\n",
      "GD iter. 119/149: loss=0.11490231855593183, w0=0.057853129889593014, w1=0.005303023072507151\n",
      "GD iter. 120/149: loss=0.11484027949595559, w0=0.05809530358394553, w1=0.005276091891138473\n",
      "GD iter. 121/149: loss=0.11477914139630109, w0=0.05833596673072132, w1=0.005248754562743506\n",
      "GD iter. 122/149: loss=0.11471889073411963, w0=0.05857513082998631, w1=0.0052210142353379735\n",
      "GD iter. 123/149: loss=0.1146595141896097, w0=0.058812807282090755, w1=0.005192874028953586\n",
      "GD iter. 124/149: loss=0.11460099864296637, w0=0.05904900738955381, w1=0.005164337036218541\n",
      "GD iter. 125/149: loss=0.11454333117137747, w0=0.05928374235883561, w1=0.005135406322901581\n",
      "GD iter. 126/149: loss=0.1144864990460642, w0=0.05951702330200617, w1=0.005106084928422669\n",
      "GD iter. 127/149: loss=0.11443048972936727, w0=0.05974886123831979, w1=0.005076375866333065\n",
      "GD iter. 128/149: loss=0.1143752908718762, w0=0.05997926709570261, w1=0.0050462821247673675\n",
      "GD iter. 129/149: loss=0.11432089030960235, w0=0.06020825171216069, w1=0.005015806666869833\n",
      "GD iter. 130/149: loss=0.11426727606119423, w0=0.06043582583711494, w1=0.004984952431197121\n",
      "GD iter. 131/149: loss=0.11421443632519498, w0=0.06066200013266901, w1=0.004953722332099399\n",
      "GD iter. 132/149: loss=0.11416235947734071, w0=0.060886785174815515, w1=0.004922119260081581\n",
      "GD iter. 133/149: loss=0.11411103406790003, w0=0.06111019145458561, w1=0.004890146082146338\n",
      "GD iter. 134/149: loss=0.11406044881905318, w0=0.061332229379146494, w1=0.004857805642120345\n",
      "GD iter. 135/149: loss=0.11401059262231036, w0=0.061552909272850904, w1=0.004825100760965139\n",
      "GD iter. 136/149: loss=0.1139614545359695, w0=0.06177224137824256, w1=0.004792034237073805\n",
      "GD iter. 137/149: loss=0.11391302378261181, w0=0.061990235857020876, w1=0.004758608846554648\n",
      "GD iter. 138/149: loss=0.11386528974663478, w0=0.06220690279096823, w1=0.004724827343502864\n",
      "GD iter. 139/149: loss=0.11381824197182291, w0=0.062422252182842634, w1=0.004690692460261163\n",
      "GD iter. 140/149: loss=0.1137718701589541, w0=0.06263629395723848, w1=0.004656206907670211\n",
      "GD iter. 141/149: loss=0.11372616416344267, w0=0.0628490379614178, w1=0.004621373375309676\n",
      "GD iter. 142/149: loss=0.11368111399301738, w0=0.06306049396611423, w1=0.004586194531730604\n",
      "GD iter. 143/149: loss=0.11363670980543464, w0=0.06327067166631169, w1=0.0045506730246797815\n",
      "GD iter. 144/149: loss=0.11359294190622576, w0=0.06347958068199978, w1=0.004514811481316692\n",
      "GD iter. 145/149: loss=0.1135498007464781, w0=0.06368723055890736, w1=0.004478612508423615\n",
      "GD iter. 146/149: loss=0.11350727692064969, w0=0.06389363076921602, w1=0.004442078692609372\n",
      "GD iter. 147/149: loss=0.11346536116441672, w0=0.06409879071225483, w1=0.004405212600507186\n",
      "GD iter. 148/149: loss=0.11342404435255274, w0=0.06430271971517766, w1=0.004368016778967069\n",
      "GD iter. 149/149: loss=0.11338331749684084, w0=0.0645054270336243, w1=0.004330493755243132\n"
     ]
    }
   ],
   "source": [
    "losses, ws = mean_squared_error_gd(y_tr, x, initial_w, max_iters=150, gamma=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b1b188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[350000 -1]\n",
      " [350001 -1]\n",
      " [350002 -1]\n",
      " [350003 -1]\n",
      " [350004 1]]\n"
     ]
    }
   ],
   "source": [
    "best_ws = ws[-1]\n",
    "pred = get_predictions(x_te, best_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17327391",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"submission.csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx)).dot(tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations. Given the large number of parameters (that may be correlated), it's better to use ridge regression or lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64755be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y âˆˆ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e0e3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating loss with sigmoid and using gradient descent\n",
    "def logistic_regression_gradient_descent(y_tr, x):\n",
    "    max_iter = 500\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y_tr.shape[0],)), x]\n",
    "    tx = x\n",
    "    initial_w = np.zeros((tx.shape[1],), dtype=float)\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y_tr, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y_tr, tx, w)))\n",
    "    \n",
    "    return losses[-1], w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a992b41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1), (250000, 20))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = np.c_[np.ones((y_tr.shape[0],)), x]\n",
    "initial_w = np.zeros((tx.shape[1],1), dtype=float)\n",
    "w = initial_w\n",
    "w.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d039f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599171\n",
      "Current iteration=100, loss=0.6113890294856259\n",
      "Current iteration=200, loss=0.5993193152531957\n",
      "Current iteration=300, loss=0.5912192725832195\n",
      "Current iteration=400, loss=0.5852477360620909\n",
      "loss=0.5806758546942088\n"
     ]
    }
   ],
   "source": [
    "losses_logreg, ws_logreg = logistic_regression_gradient_descent(y_tr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0abae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_logistic(x, best_w):\n",
    "    preds = sigmoid(x.dot(best_w).reshape((x.shape[0],1)))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[:, 0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7df0cc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ws = ws_logreg\n",
    "best_ws.shape\n",
    "#pred = get_predictions_logistic(x_te, best_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db72eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[350000 -1]\n",
      " [350001 -1]\n",
      " [350002 -1]\n",
      " [350003 -1]\n",
      " [350004 -1]]\n"
     ]
    }
   ],
   "source": [
    "pred = get_predictions_logistic(x = x_te, best_w = best_ws)\n",
    "np.savetxt(\"submission2.csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1324494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 2000\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y_tr.shape[0],)), x]\n",
    "    initial_w = np.zeros((tx.shape[1],), dtype=float)\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return losses[-1], w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_p, ws_p = logistic_regression_penalized_gradient_descent(y_tr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    gamma = 1.\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], )), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    return losses, w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nm, ws_nm = logistic_regression_newton_method(y_tr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f6f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
