{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452c81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (250000, 30)  y:  (250000,)\n"
     ]
    }
   ],
   "source": [
    "y = train[1]\n",
    "x_tr = train[2:].T\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11c0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_float = x_tr.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf7610b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_float[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b094bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced=replace_with_median(x_tr_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c01168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999.0 112.406\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_float[2,0],replaced[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "370c2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_VIF=colinearity_check(x_tr_float,4)\n",
    "features_VIF_replaced=colinearity_check(replaced,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f05e05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features x-y' 'VIF value' '4-10' '6.524101144326354' '4-22'\n",
      " '5.1596893608980885' '4-30' '5.778013281848558' '5-6'\n",
      " '19.047669981172188' '5-7' '53844.07915228592' '5-13' '619469.2963445598'\n",
      " '5-23' '8.083860994944821' '5-27' '1530.4175157085713' '5-28'\n",
      " '197248.12895290542' '5-29' '224386.1953107767' '5-30' '4.06157170901049'\n",
      " '6-7' '18.51373741116583' '6-13' '18.890975756598333' '6-23'\n",
      " '5.933071846449745' '6-27' '19.61629470863673' '6-28'\n",
      " '18.864456216699192' '6-29' '18.86493411790321' '6-30'\n",
      " '4.152349206013538' '7-13' '96849.9011364278' '7-23' '8.096334811780176'\n",
      " '7-27' '1530.8949719738653' '7-28' '83179.90808920997' '7-29'\n",
      " '87773.28208721284' '7-30' '4.063609786733423' '10-22'\n",
      " '10.994249005835202' '10-23' '4.709509835921955' '10-30'\n",
      " '29.602520510248585' '13-23' '8.088287488606015' '13-27'\n",
      " '1550.0017316503333' '13-28' '330686.36326640844' '13-29'\n",
      " '409261.75027636247' '13-30' '4.062998350213046' '22-30'\n",
      " '9.182150549610979' '23-24' '6.515615502517554' '23-25'\n",
      " '6.025504079174276' '23-26' '6.025720072438802' '23-27'\n",
      " '8.20479221534431' '23-28' '8.08908774107135' '23-29' '8.088870937543987'\n",
      " '23-30' '5.664754103299299' '24-25' '256.91203953546426' '24-26'\n",
      " '256.8978004137104' '25-26' '122502.87778944003' '27-28'\n",
      " '1544.5386969749165' '27-29' '1543.356431279963' '27-30'\n",
      " '4.268957712045245' '28-29' '191351.4616625482' '28-30'\n",
      " '4.063113903965437' '29-30' '4.062997957027839']\n"
     ]
    }
   ],
   "source": [
    "print(features_VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e704bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features x-y' 'VIF value' '1-3' '5.686226789480798' '4-10'\n",
      " '6.524101144326354' '4-22' '5.1596893608980885' '4-24'\n",
      " '4.840687535901952' '4-30' '5.778013281848558' '5-6' '5.455266031311051'\n",
      " '5-7' '6.758915407980832' '6-7' '4.775765880064227' '10-22'\n",
      " '10.994249005835202' '10-23' '4.709509835921955' '10-24'\n",
      " '5.76077498400385' '10-30' '29.602520510248585' '22-24'\n",
      " '4.247982828249272' '22-30' '9.182150549610979' '23-30'\n",
      " '5.664754103299299' '24-30' '5.637740057190627']\n"
     ]
    }
   ],
   "source": [
    "print(features_VIF_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)  and y[:5]:  [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_tr = np.where(y == \"s\",1,0)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 19), (568238, 19))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9135d0",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # print w and loss\n",
    "        #print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        #print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229356",
   "metadata": {},
   "source": [
    "#### Build polimonial basis function which can be used with Least Squares and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a593cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "        \n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x),1))\n",
    "    for j in range( 1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, j)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5430e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_ls(y, x, degrees=[1, 3, 7, 12]):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    ws = []\n",
    "    losses = []\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(x, degree)\n",
    "        w, loss = least_squares(y, tx)\n",
    "        rmse = np.sqrt(2 * loss)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(rmse)\n",
    "\n",
    "        #print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "         #     i=ind + 1, d=degree, loss=rmse))\n",
    "    ind = argmin(losses)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64755be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecad3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # TODO split and add test data\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(compute_rmse(y_tr, tx_tr, weight))\n",
    "        \n",
    "        #print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "         #      p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    ind = argmin(rmse_tr)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y ∈ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca38bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    max_iter = 500\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f0bae",
   "metadata": {},
   "source": [
    "#### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b8018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 2000\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y_tr.shape[0],)), x]\n",
    "    initial_w = np.zeros((tx.shape[1],), dtype=float)\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    gamma = 1.\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], )), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3eec7",
   "metadata": {},
   "source": [
    "## Step 4 - Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_w).reshape((x.shape[0],))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    np.savetxt(title + \".csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e218a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "MSE- GD Loss:  0.11338331749684086\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Gradient Descent\n",
    "w_gd, loss_gd = mean_squared_error_gd(y_tr, x, initial_w, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_gd)\n",
    "print(\"MSE - GD Loss: \", loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cbc6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "MSE - SGD Loss:  0.11336016574552966\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Stochastic Gradient Descent\n",
    "w_sgd, loss_sgd = mean_squared_error_sgd(y_tr, x, initial_w, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_sgd)\n",
    "print(\"MSE - SGD Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "639ca13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n",
      "MSE - LS Loss:  0.11336016574552966\n"
     ]
    }
   ],
   "source": [
    "# Least Squares\n",
    "w_ls, loss_ls = least_squares(y_tr, x)\n",
    "#w_poly, loss_poly = polynomial_regression_ls(y_tr, x)\n",
    "pred = get_predictions(x_te, w_ls)\n",
    "print(\"MSE - LS Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06cc5f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "# TODO: add split data\n",
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "#w_rr, loss_rr = ridge_regression_demo(x, y, degree, split_ratio, seed)\n",
    "w_rr = ridge_regression(y_tr, x, .0005 )\n",
    "pred_rr = get_predictions(x_te, w_rr)\n",
    "#print(\"MSE - RR Loss: \", loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa7a9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599341\n",
      "Current iteration=100, loss=0.611389029485626\n",
      "Current iteration=200, loss=0.5993193152531958\n",
      "Current iteration=300, loss=0.5912192725832195\n",
      "Current iteration=400, loss=0.5852477360620911\n",
      "loss=0.5806758546942089\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Logictic Regression Gradient Descent\u001b[39;00m\n\u001b[1;32m      2\u001b[0m w_logreg, loss_logreg \u001b[38;5;241m=\u001b[39m logistic_regression_gradient_descent(y_tr, x)\n\u001b[0;32m----> 3\u001b[0m pred_log \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m(x_te, w_logreg)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE - LogRed Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_logreg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Logictic Regression Gradient Descent\n",
    "w_logreg, loss_logreg = logistic_regression_gradient_descent(y_tr, x)\n",
    "pred_log = get_predictions(x_te, w_logreg)\n",
    "print(\"RMSE - LogRed Loss: \", loss_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression with GD\n",
    "w_reglog, loss_reglog = logistic_regression_regularized_gradient_descent(y_tr, x)\n",
    "pred_reglog = get_predictions(x_te, w_reglog)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e002c473d40394036330ccbc573ff2124176c3c3fbf7652b00c2ff8632cfa48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
