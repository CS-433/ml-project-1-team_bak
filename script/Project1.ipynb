{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c33a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "y, x, ids = load_csv_data('train.csv')\n",
    "_, x_test, ids_test = load_csv_data('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ae6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, x.shape, ids.shape, x_test.shape, ids_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae08e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(x_train, x_test, alpha=0):\n",
    "    \"\"\"\n",
    "    Preprocessing: \n",
    "    - impute missing values using median,\n",
    "    - feature study: see \"plots\" file,\n",
    "    - impute outliers using alpha-percentiles,\n",
    "    - standardization\n",
    "    \"\"\"\n",
    "    # Missing Values: \n",
    "    \n",
    "    # Consider the 0s in the 'PRI_jet_all_pt' as missing values\n",
    "    x_train[:,-1]=np.where(x_train[:,-1]==0, -999, x_train[:,-1])\n",
    "    \n",
    "    # Impute missing data\n",
    "    x_train, x_test = impute_missing(x_train, x_test) # see the impute_missing function in helpfulfun\n",
    "    \n",
    "    # Feature study:\n",
    "    # Delete useless features\n",
    "    x_train = np.delete(x_train, [15,16,18,19,20,21], 1) # rimuove features inutili, guardare file plot per capire\n",
    "    x_test = np.delete(x_test, [15,16,18,19,20,21], 1)\n",
    "    \n",
    "    # Impute outliers\n",
    "    x_train = outliers(x_train, alpha) # rimuove gli outliers giudicando i percentili\n",
    "    x_test = outliers(x_test, alpha)\n",
    "    \n",
    "    # Standardization\n",
    "    x_train, mean_x_train, std_x_train = standardize(x_train) # standardizza i dati.. come abiam fatto noi\n",
    "    x_test, _, _ = standardize(x_test, mean_x_train, std_x_train)\n",
    "     \n",
    "    return x_train, x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_parameters_ridge_regression_jet(y,x,degrees,lambdas,alphas,k_fold,seed):\n",
    "    \"\"\"\n",
    "    Given the training set and a set of tuples of parameters (alphas, lamdas, degrees) \n",
    "    for each jet_subset returns the tuple which maximize the accuracy predicted through Cross Validation \n",
    "    \"\"\"\n",
    "    par_degree = []\n",
    "    par_lamb = []\n",
    "    par_alpha = []\n",
    "    accuracy = []\n",
    "\n",
    "    # Split the training set in subsets according to the jet value \n",
    "    jet_class = {\n",
    "        0: x[:, 22] == 0,\n",
    "        1: x[:, 22] == 1,\n",
    "        2: x[:, 22] == 2, \n",
    "        3: x[:, 22] == 3\n",
    "        }\n",
    "\n",
    "    for idx in range(len(jet_class)):\n",
    "        x_jet = x[jet_class[idx]]\n",
    "        y_jet = y[jet_class[idx]]\n",
    "        \n",
    "        degree,lamb,alpha,accu = select_parameters_ridge_regression(degrees, lambdas, alphas, k_fold, y_jet, x_jet, seed)\n",
    "        par_degree.append(degree)\n",
    "        par_lamb.append(lamb)\n",
    "        par_alpha.append(alpha)\n",
    "        accuracy.append(accu)\n",
    "\n",
    "    return par_degree, par_lamb, par_alpha, accuracy\n",
    "\n",
    "def select_parameters_ridge_regression(degrees, lambdas, alphas, k_fold, y, x, seed):\n",
    "    \"\"\"\n",
    "    Given the training set and a set of tuples of parameters (alphas, lamdas, degrees) \n",
    "    returns the tuple which maximize the accuracy predicted through Cross Validation \n",
    "    \"\"\"\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    best_param = []\n",
    "\n",
    "    for degree in degrees:\n",
    "        for lamb in lambdas:\n",
    "            for alpha in alphas:\n",
    "                accuracy_test = []\n",
    "                for k in range(k_fold):\n",
    "                        _, acc_test = cross_validation(y, x, ridge_regression, k_indices, k, degree, alpha, lamb)\n",
    "                        accuracy_test.append(acc_test)\n",
    "                best_param.append([degree,lamb,alpha,np.mean(accuracy_test)])\n",
    "    \n",
    "    best_param = np.array(best_param)\n",
    "    ind_best =  np.argmax(best_param[:,3])  #param that maximizes the accuracy    \n",
    "    best_degree = best_param[ind_best,0]\n",
    "    best_lamb = best_param[ind_best,1]\n",
    "    best_alpha = best_param[ind_best,2]\n",
    "    accu = best_param[ind_best,3]\n",
    "   \n",
    "    return best_degree, best_lamb, best_alpha, accu\n",
    "\n",
    "def cross_validation(y, x, method, k_indices, k, degree, alpha, lamb=None, log=False, **kwargs):\n",
    "    \"\"\"k-fold cross-validation for the different methods: LS with GD, LS with SGD, Normal Equations, Logistic and Regularized Logistic Regression with SGD\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    test_indeces = k_indices[k] # molto semplicemente prende il gruppo k-esimo e lo mette come test\n",
    "    train_indeces = np.delete(k_indices, (k), axis=0).ravel() # qua invece prende tutti gli altri gruppi e li usa come train\n",
    "\n",
    "    x_train = x[train_indeces, :] # crea i data set\n",
    "    x_test = x[test_indeces, :]\n",
    "    y_train = y[train_indeces] \n",
    "    y_test = y[test_indeces] \n",
    "\n",
    "    # initialize output vectors\n",
    "    y_train_pred = np.zeros(len(y_train)) # crea due vettori vuoti che poi conterranno le previsioni per il train e per il test\n",
    "    y_test_pred = np.zeros(len(y_test))\n",
    " \n",
    "    # data pre-processing\n",
    "    x_train, x_test = pre_process_data(x_train, x_test, alpha) # qui fa il preprocessing \n",
    "            \n",
    "    # transformation\n",
    "    x_train = build_poly(x_train, degree) \n",
    "    x_test = build_poly(x_test, degree) \n",
    "        \n",
    "    # compute weights using given method\n",
    "    if lamb == None:\n",
    "        weights, _ = method(y_train, x_train, **kwargs) \n",
    "    else: \n",
    "        weights, _ = method(y_train, x_train, lamb, **kwargs) # ridge regression in this case\n",
    "       \n",
    "    # predict\n",
    "    if log == True: # quindi se abbiamo a che fare con logistic\n",
    "        y_train_pred = predict_labels_logistic(weights, x_train) # applica la funzione predict_labels_logistic\n",
    "        y_test_pred = predict_labels_logistic(weights, x_test)\n",
    "        print(y_train_pred, y_train)\n",
    "    else:\n",
    "        y_train_pred = transform_binary(weights, x_train) # se non è una logistic regression applica predict_labels\n",
    "        y_test_pred = transform_binary(weights, x_test)\n",
    "        \n",
    "\n",
    "    # compute accuracy for train and test data\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train) # qua applica compute_accuracy function\n",
    "                                                           \n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test\n",
    "\n",
    "\n",
    "def cross_validation_jet(y, x, method, k_indices, k, degrees, alphas, lambdas=None, log=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Completes k-fold cross-validation for Least Squares with GD, SGD, Normal Equations, Logistic and Regularized Logistic \n",
    "    Regression with SGD\n",
    "    \"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    test_indeces = k_indices[k] # molto semplicemente prende il gruppo k-esimo e lo mette come test\n",
    "    train_indeces = np.delete(k_indices, (k), axis=0).ravel() # qua invece prende tutti gli altri gruppi e li usa come train\n",
    "\n",
    "    x_train_all_jets = x[train_indeces, :]\n",
    "    x_test_all_jets = x[test_indeces, :]\n",
    "    y_train_all_jets = y[train_indeces]\n",
    "    y_test_all_jets = y[test_indeces]\n",
    "\n",
    "    # split in 4 subsets the training set accordingly to JET class\n",
    "    jet_train_class = {\n",
    "        0: x_train_all_jets[:, 22] == 0,\n",
    "        1: x_train_all_jets[:, 22] == 1,\n",
    "        2: x_train_all_jets[:, 22] == 2, \n",
    "        3: x_train_all_jets[:, 22] == 3\n",
    "    }\n",
    "    \n",
    "    jet_test_class = {\n",
    "        0: x_test_all_jets[:, 22] == 0,\n",
    "        1: x_test_all_jets[:, 22] == 1,\n",
    "        2: x_test_all_jets[:, 22] == 2, \n",
    "        3: x_test_all_jets[:, 22] == 3\n",
    "    }\n",
    "\n",
    "\n",
    "    # initialize output vectors\n",
    "    y_train_pred = np.zeros(len(y_train_all_jets))\n",
    "    y_test_pred = np.zeros(len(y_test_all_jets))\n",
    "\n",
    "    for idx in range(len(jet_train_class)):\n",
    "        x_train = x_train_all_jets[jet_train_class[idx]]\n",
    "        x_test = x_test_all_jets[jet_test_class[idx]]\n",
    "        y_train = y_train_all_jets[jet_train_class[idx]]\n",
    "\n",
    "        # data pre-processing\n",
    "        x_train, x_test = pre_process_data(x_train, x_test, alphas[idx])\n",
    "        x_train = build_poly(x_train, degrees[idx]) \n",
    "        x_test = build_poly(x_test, degrees[idx]) \n",
    "        \n",
    "        # compute weights using given method\n",
    "        if lambdas == None:\n",
    "            weights, _ = method(y_train, x_train, **kwargs)\n",
    "        else:\n",
    "            weights, _ = method(y_train, x_train, lambdas[idx], **kwargs)\n",
    "        \n",
    "        # predict\n",
    "        if log == True:\n",
    "            y_train_pred[jet_train_class[idx]] = predict_labels_logistic(weights, x_train)\n",
    "            y_test_pred[jet_test_class[idx]] = predict_labels_logistic(weights, x_test)\n",
    "        else:\n",
    "            y_train_pred[jet_train_class[idx]] = predict_labels(weights, x_train)\n",
    "            y_test_pred[jet_test_class[idx]] = predict_labels(weights, x_test)\n",
    "        \n",
    "    # compute accuracy for train and test data\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train_all_jets)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test_all_jets)\n",
    "    \n",
    "    return acc_train, acc_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# canditates parameters\n",
    "degrees_candidates = [2,2,4,5,6,7,8,9,10]\n",
    "alphas_candidates=[2,3,4,5,6,7,8,9]\n",
    "lambdas_candidates = [1e-02, 1e-03, 1e-06, 1e-07]\n",
    "\n",
    "\n",
    "k_fold = 3\n",
    "\n",
    "opt_degree, opt_lambda, opt_alpha, accuracy = select_parameters_ridge_regression_jet(y,x,degrees_candidates,lambdas_candidates,\n",
    "                                                                  alphas_candidates,k_fold,seed)\n",
    "print('Optimal alphas per jet_class:',opt_alpha)\n",
    "print('Optimal degrees per jet_class:',opt_degree)\n",
    "print('Optimal lambdas per jet_class:',opt_lambda)\n",
    "print('Maximum accuracy predicted per jet_class:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da998ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [3, 3, 6, 6]\n",
    "alphas = [7, 9, 5, 5]\n",
    "lambdas = [0.00021544346900318823, 1e-07, 4.641588833612773e-06, 0.00021544346900318823]\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 3\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, x, ridge_regression, k_indices, k, degrees, alphas, lambdas)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "degrees = [3, 3, 6, 6]\n",
    "alphas = [7, 9, 5, 5]\n",
    "lambdas = [0.00021544346900318823, 1e-07, 4.641588833612773e-06, 0.00021544346900318823]\n",
    "\n",
    "\n",
    "jet_train_class = {\n",
    "    0: x[:, 22] == 0,\n",
    "    1: x[:, 22] == 1,\n",
    "    2: x[:, 22] == 2, \n",
    "    3: x[:, 22] == 3\n",
    "}\n",
    "\n",
    "    \n",
    "jet_test_class = {\n",
    "    0: x_test[:, 22] == 0,\n",
    "    1: x_test[:, 22] == 1,\n",
    "    2: x_test[:, 22] == 2, \n",
    "    3: x_test[:, 22] == 3\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    x_jet = x[jet_train_class[i]]\n",
    "    x_jet_test = x_test[jet_test_class[i]]\n",
    "    y_jet = y[jet_train_class[i]]\n",
    "\n",
    "    # Pre-processing and transformation of the training set and test set\n",
    "    x_jet, x_jet_test = pre_process_data(x_jet, x_jet_test, alphas[i])\n",
    "    x_jet = build_poly(x_jet, degrees[i])\n",
    "    x_jet_test = build_poly(x_jet_test, degrees[i])\n",
    "    \n",
    "    # Train the model through Ridge Regression\n",
    "    best_w, _ = ridge_regression(y_jet, x_jet, lambdas[i])\n",
    "    \n",
    "    # Prediction\n",
    "    pred = transform_binary(best_w, x_jet_test)\n",
    "    ridge_pred[jet_test_class[i]] = pred\n",
    "\n",
    "ridge_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    y_pred = np.c_[ids_test, pred].astype(str)\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    np.savetxt(title + \".csv\", y_pred, fmt=\"%s\", delimiter=\",\")\n",
    "\n",
    "savePredictions(ridge_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7e0f0",
   "metadata": {},
   "source": [
    "# End of new part. Previous work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import the datasets\n",
    "\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ce9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[1]\n",
    "x_tr = train[2:].T\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_float = x_tr.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_float[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced=replace_with_median(x_tr_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c01168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr_float[2,0],replaced[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_VIF=colinearity_check(x_tr_float,4)\n",
    "features_VIF_replaced=colinearity_check(replaced,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_VIF_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train[1]\n",
    "y_tr = np.where(y == \"s\",1,-1)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94764b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_2 = np.genfromtxt(\"x_tr_2.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_2 = np.array(x_tr_2).T\n",
    "\n",
    "x_te_2 = np.genfromtxt(\"x_te_2.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_2 = np.array(x_te_2).T\n",
    "\n",
    "x_tr_3 = np.genfromtxt(\"x_tr_3_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_3 = np.array(x_tr_3).T\n",
    "\n",
    "x_te_3 = np.genfromtxt(\"x_te_3_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_3 = np.array(x_te_3).T\n",
    "\n",
    "x_tr_4 = np.genfromtxt(\"x_tr_4_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_tr_4 = np.array(x_tr_4).T\n",
    "x_te_4 = np.genfromtxt(\"x_te_4_no_out.csv\", dtype=float, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "x_te_4 = np.array(x_te_4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec703350",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_4.shape, x_te_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41664a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_4)\n",
    "x_te, _, _ = standardize(x_te_4)\n",
    "#x = x_tr_2\n",
    "#x_te = x_te_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef398d78",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # print w and loss\n",
    "        #print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = np.zeros((x.shape[1],), dtype=float) #initial w\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        #print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229356",
   "metadata": {},
   "source": [
    "#### Build polimonial basis function which can be used with Least Squares and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a593cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "        \n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x),1))\n",
    "    for j in range( 1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, j)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5430e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_ls(y, x, degrees=[1, 3, 7, 12]):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    ws = []\n",
    "    losses = []\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(x, degree)\n",
    "        w, loss = least_squares(y, tx)\n",
    "        rmse = np.sqrt(2 * loss)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(rmse)\n",
    "\n",
    "        #print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "         #     i=ind + 1, d=degree, loss=rmse))\n",
    "    ind = argmin(losses)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # TODO split and add test data\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(compute_rmse(y_tr, tx_tr, weight))\n",
    "        \n",
    "        #print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "         #      p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    ind = argmin(rmse_tr)\n",
    "    return ws[ind], losses[ind]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y ∈ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    max_iter = 1200\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f0bae",
   "metadata": {},
   "source": [
    "#### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_regularized_gradient_descent(y, x, gamma):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y, tx, w)))\n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    gamma = 1.\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], )), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\", True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3eec7",
   "metadata": {},
   "source": [
    "## Step 4 - Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_w).reshape((x.shape[0],))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    np.savetxt(title + \".csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e218a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error Gradient Descent\n",
    "w_gd, loss_gd = mean_squared_error_gd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_gd)\n",
    "print(\"MSE - GD Loss: \", loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Stochastic Gradient Descent\n",
    "w_sgd, loss_sgd = mean_squared_error_sgd(y_tr, x, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_sgd)\n",
    "print(\"MSE - SGD Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ca13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Squares\n",
    "w_ls, loss_ls = least_squares(y_tr, x)\n",
    "#w_poly, loss_poly = polynomial_regression_ls(y_tr, x)\n",
    "pred = get_predictions(x_te, w_ls)\n",
    "print(\"MSE - LS Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "# TODO: add split data\n",
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "#w_rr, loss_rr = ridge_regression_demo(x, y, degree, split_ratio, seed)\n",
    "w_rr = ridge_regression(y_tr, x, .0005 )\n",
    "pred_rr = get_predictions(x_te, w_rr)\n",
    "#print(\"MSE - RR Loss: \", loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logictic Regression Gradient Descent\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_logreg, loss_logreg = logistic_regression_gradient_descent(y_tr, x)\n",
    "pred_log = get_predictions(x_te, w_logreg)\n",
    "print(\"RMSE - LogRed Loss: \", loss_logreg)\n",
    "y_tr = np.where(y == \"s\",1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f74c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_log, title=\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression with GD\n",
    "y_tr = np.where(y == \"s\",1,0)\n",
    "w_reglog, loss_reglog = logistic_regression_regularized_gradient_descent(y_tr, x, .001)\n",
    "pred_reglog = get_predictions(x_te, w_reglog)\n",
    "# submission : 0.680 dont know why loss is increasing but prediction got better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePredictions(pred_reglog, title=\"submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51db59b",
   "metadata": {},
   "source": [
    "## Step - 5 : Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(method, y, x, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\"\"\"\n",
    "    \n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    \n",
    "    \n",
    "    if method == \"ridge_regression_demo\":\n",
    "        # form data with polynomial degree\n",
    "        tx_tr = build_poly(x_tr, degree)\n",
    "        tx_te = build_poly(x_te, degree)\n",
    "        # ridge regression\n",
    "        best_w = ridge_regression(y_tr, tx_tr, lambda_ridge)\n",
    "        loss_tr = sum(get_predictions_cv(tx_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(tx_te, best_w) == y_te)/len(y_te)\n",
    "        #y_pred = get_predictions_cv(tx_te, best_w)\n",
    "        \n",
    "    if method == \"least_squares\":\n",
    "        best_w, loss_ls = least_squares(y_tr, x_tr)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)\n",
    "        \n",
    "    if method == \"mean_squared_error_gd\":\n",
    "        best_w, loss = mean_squared_error_gd(y_tr, x_tr, max_iters = 150, gamma = gamma)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)\n",
    "        \n",
    "    if method == \"mean_squared_error_sgd\":\n",
    "        best_w, loss = mean_squared_error_sgd(y_tr, x_tr, max_iters=150, gamma = gamma)\n",
    "        loss_tr = sum([get_predictions_cv(x_tr, best_w) == y_tr])/len(y_tr)\n",
    "        loss_te = sum([get_predictions_cv(x_te, best_w) == y_te])/len(y_te)    \n",
    "\n",
    "    if method == \"logistic_regression_gradient_descent\":\n",
    "        best_w, loss_logreg = logistic_regression_gradient_descent(y_tr, x_tr)\n",
    "        loss_tr = sum(get_predictions_cv(x_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(x_te, best_w) == y_te)/len(y_te)           \n",
    "        \n",
    "    if method == \"logistic_regression_regularized_gradient_descent\":\n",
    "        best_w, loss_logreg = logistic_regression_regularized_gradient_descent(y_tr, x_tr, lambda_logistic)\n",
    "        loss_tr = sum(get_predictions_cv(x_tr, best_w) == y_tr)/len(y_tr)\n",
    "        loss_te = sum(get_predictions_cv(x_te, best_w) == y_te)/len(y_te)                   \n",
    "        \n",
    "    return loss_tr, loss_te, best_w   #, y_pred, y_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(method, y, tx, max_iters, gamma, lambdas_ridge, lambdas_logistic):\n",
    "    \n",
    "    if method in (\"mean_squared_error_gd\", \"mean_squared_error_sgd\", \n",
    "                  \"least_squares\", \"logistic_regression_gradient_descent\"):\n",
    "        seed = 7\n",
    "        k_fold = 10\n",
    "        #split data in k fold \n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0 \n",
    "        lambda_logistic = 0\n",
    "        degree = 0\n",
    "        # cross validation:\n",
    "        for k in range(k_fold):\n",
    "            print(\"k fold = \", k+1 , \"/\", k_fold)\n",
    "            loss_tr, loss_te,_ = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "            losses_tr.append(loss_tr)\n",
    "            losses_te.append(loss_te)\n",
    "            \n",
    "        loss_tr = np.mean(losses_tr)\n",
    "        loss_te = np.mean(losses_te)\n",
    "        \n",
    "        return (loss_tr, loss_te)\n",
    "            \n",
    "        \n",
    "    #if method == \"polynomial_regression_ls\":\n",
    "        #####\n",
    "        \n",
    "    if method == \"ridge_regression_demo\":\n",
    "        seed = 7\n",
    "        degree = 7\n",
    "        k_fold = 10\n",
    "        # split data in k fold\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0\n",
    "        degree = 0\n",
    "        # cross validation\n",
    "        step = 1\n",
    "        for lambda_ridge in lambdas_ridge:\n",
    "            losses_tr_tmp = []\n",
    "            losses_te_tmp = []\n",
    "            print(step, \"/\" , len(lambdas_ridge))\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te,_  = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "                losses_tr_tmp.append(loss_tr)\n",
    "                losses_te_tmp.append(loss_te)\n",
    "                print(\"step : \", step, \"k_fold : \", k,\"/\", k_fold)\n",
    "            losses_tr.append(np.mean(losses_tr_tmp))\n",
    "            losses_te.append(np.mean(losses_te_tmp))\n",
    "            step += 1\n",
    "            \n",
    "\n",
    "        #cross_validation_visualization(lambdas, losses_tr, losses_te)\n",
    "        print(\"losses train = \", losses_tr, \"\\n\\n\", \"losses test = \", losses_te)\n",
    "        return (losses_tr, losses_te)\n",
    "        \n",
    "        \n",
    "    if method == \"logistic_regression_regularized_gradient_descent\":\n",
    "        seed = 7\n",
    "        k_fold = 10\n",
    "        # split data in k fold\n",
    "        k_indices = build_k_indices(y, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        lambda_ridge = 0\n",
    "        degree = 0\n",
    "        # cross validation\n",
    "        step = 1\n",
    "        for lambda_logistic in lambdas_logistic:\n",
    "            losses_tr_tmp = []\n",
    "            losses_te_tmp = []\n",
    "            print(step, \"/\" , len(lambdas_logistic))\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te,_  = cross_validation(method,y, tx, k_indices, k, gamma, lambda_ridge, degree, lambda_logistic)\n",
    "                losses_tr_tmp.append(loss_tr)\n",
    "                losses_te_tmp.append(loss_te)\n",
    "                print(\"step : \", step, \"k_fold : \", k+1,\"/\", k_fold)\n",
    "            losses_tr.append(np.mean(losses_tr_tmp))\n",
    "            losses_te.append(np.mean(losses_te_tmp))\n",
    "            step += 1\n",
    "        print(\"losses train = \", losses_tr, \"\\n\\n\", \"losses test = \", losses_te)\n",
    "        return (losses_tr, losses_te)\n",
    "        \n",
    "    #if method == \"logistic_regression_newton_method\":\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tr = np.where(y == \"s\",1,-1)\n",
    "# k_fold_cross_validation(\"ridge_regression_demo\", y_tr, x,  150, .5, (4,2,1,.5), (4,2,1,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94babe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_tr = np.where(y == \"s\",1,0)\n",
    "k_fold_cross_validation(\"logistic_regression_gradient_descent\", y_tr, x,  150, .5, (4,2,1,.5), (4,2,1,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_cross_validation(\"logistic_regression_regularized_gradient_descent\", \n",
    "                        y_tr, x,  150, .5, (4,2,1,.5), (0.001, 0.1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c18eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e002c473d40394036330ccbc573ff2124176c3c3fbf7652b00c2ff8632cfa48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
