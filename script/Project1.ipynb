{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452c81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (250000, 30)  y:  (250000,)\n"
     ]
    }
   ],
   "source": [
    "y = train[1]\n",
    "x_tr = train[2:].T\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)  and y[:5]:  [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_tr = np.where(y == \"s\",1,0)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9135d0",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_ws).reshape((x.shape[0],))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[0], y_te]\n",
    "    print(y_pred[0])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e218a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/149: loss=0.171334, w0=0.0017133399999999998, w1=0.00043645910164310744\n",
      "GD iter. 1/149: loss=0.16499256885493627, w0=0.0033209621458787815, w1=0.0008386390404596991\n",
      "GD iter. 2/149: loss=0.159648601737275, w0=0.004831742599413398, w1=0.0012094336335768635\n",
      "GD iter. 3/149: loss=0.15513735265013393, w0=0.0062537896663250065, w1=0.0015514862427284625\n",
      "GD iter. 4/149: loss=0.1513214094005602, w0=0.007594510397486251, w1=0.0018672114987907773\n",
      "GD iter. 5/149: loss=0.1480861566518639, w0=0.008860671412039433, w1=0.002158815141566248\n",
      "GD iter. 6/149: loss=0.14533599206028716, w0=0.010058454443722411, w1=0.0024283121383331407\n",
      "GD iter. 7/149: loss=0.14299117049171758, w0=0.011193507068207447, w1=0.0026775432304924408\n",
      "GD iter. 8/149: loss=0.14098517206359557, w0=0.01227098902953978, w1=0.0029081900446875147\n",
      "GD iter. 9/149: loss=0.1392625070624069, w0=0.013295614547490146, w1=0.003121788892940366\n",
      "GD iter. 10/149: loss=0.13777688521990059, w0=0.014271690954509888, w1=0.003319743375543079\n",
      "GD iter. 11/149: loss=0.1364896888682335, w0=0.015203153980725609, w1=0.003503335890575274\n",
      "GD iter. 12/149: loss=0.13536869953327263, w0=0.016093599977783278, w1=0.0036737381449067244\n",
      "GD iter. 13/149: loss=0.13438703589793835, w0=0.016946315347121457, w1=0.0038320207533144605\n",
      "GD iter. 14/149: loss=0.1335222680503527, w0=0.017764303415212068, w1=0.003979162004827861\n",
      "GD iter. 15/149: loss=0.13275567875534616, w0=0.01855030897726489, w1=0.004116055868551477\n",
      "GD iter. 16/149: loss=0.13207164734497903, w0=0.01930684071167528, w1=0.004243519304947057\n",
      "GD iter. 17/149: loss=0.13145713587460534, w0=0.020036191649945245, w1=0.004362298942831796\n",
      "GD iter. 18/149: loss=0.130901260569476, w0=0.020740457870781003, w1=0.004473077177122034\n",
      "GD iter. 19/149: loss=0.1303949344045547, w0=0.021421555572433856, w1=0.004576477737577372\n",
      "GD iter. 20/149: loss=0.1299305690101918, w0=0.022081236663984538, w1=0.004673070774440147\n",
      "GD iter. 21/149: loss=0.1295018260562017, w0=0.02272110300406435, w1=0.004763377502883426\n",
      "GD iter. 22/149: loss=0.12910340990146624, w0=0.023342619404358434, w1=0.0048478744445443625\n",
      "GD iter. 23/149: loss=0.1287308946594386, w0=0.023947125505055955, w1=0.0049269973010989355\n",
      "GD iter. 24/149: loss=0.12838057996689273, w0=0.02453584662011443, w1=0.005001144491801336\n",
      "GD iter. 25/149: loss=0.12804937069150152, w0=0.025109903641714688, w1=0.0050706803841416865\n",
      "GD iter. 26/149: loss=0.12773467660467508, w0=0.025670322085528784, w1=0.005135938244246437\n",
      "GD iter. 27/149: loss=0.12743432870565924, w0=0.026218040351341745, w1=0.0051972229313358865\n",
      "GD iter. 28/149: loss=0.127146509432985, w0=0.026753917267101038, w1=0.005254813358443829\n",
      "GD iter. 29/149: loss=0.12686969445814023, w0=0.0272787389785616, w1=0.005308964739677815\n",
      "GD iter. 30/149: loss=0.12660260413896257, w0=0.02779322524130077, w1=0.005359910642539227\n",
      "GD iter. 31/149: loss=0.1263441630293661, w0=0.028298035166951743, w1=0.005407864862215647\n",
      "GD iter. 32/149: loss=0.12609346610816247, w0=0.028793772471005948, w1=0.0054530231332907035\n",
      "GD iter. 33/149: loss=0.12584975061170248, w0=0.02928099026542659, w1=0.005495564692976602\n",
      "GD iter. 34/149: loss=0.12561237254018912, w0=0.029760195435564157, w1=0.00553565370875077\n",
      "GD iter. 35/149: loss=0.12538078706190692, w0=0.03023185263743832, w1=0.005573440582160503\n",
      "GD iter. 36/149: loss=0.12515453216838207, w0=0.030696387948321986, w1=0.005609063139538869\n",
      "GD iter. 37/149: loss=0.1249332150408753, w0=0.031154192200705602, w1=0.005642647719443069\n",
      "GD iter. 38/149: loss=0.12471650067818373, w0=0.0316056240271105, w1=0.005674310165775241\n",
      "GD iter. 39/149: loss=0.12450410241041954, w0=0.032051012640836714, w1=0.005704156734768355\n",
      "GD iter. 40/149: loss=0.12429577398574117, w0=0.03249066037555454, w1=0.00573228492330992\n",
      "GD iter. 41/149: loss=0.12409130296896542, w0=0.03292484500466139, w1=0.005758784225427912\n",
      "GD iter. 42/149: loss=0.12389050523432778, w0=0.03335382185951039, w1=0.0057837368231712614\n",
      "GD iter. 43/149: loss=0.12369322037079715, w0=0.033777825763959705, w1=0.005807218217576524\n",
      "GD iter. 44/149: loss=0.12349930784849386, w0=0.03419707280117744, w1=0.005829297804918565\n",
      "GD iter. 45/149: loss=0.12330864381989988, w0=0.03461176192725478, w1=0.0058500394029921444\n",
      "GD iter. 46/149: loss=0.12312111845051471, w0=0.03502207644491724, w1=0.0058695017317594465\n",
      "GD iter. 47/149: loss=0.12293663369109728, w0=0.035428185349471085, w1=0.005887738852322498\n",
      "GD iter. 48/149: loss=0.12275510141821817, w0=0.03583024455806882, w1=0.005904800567835959\n",
      "GD iter. 49/149: loss=0.12257644188200732, w0=0.036228398032416155, w1=0.005920732789662087\n",
      "GD iter. 50/149: loss=0.12240058241012919, w0=0.03662277880416466, w1=0.00593557787178323\n",
      "GD iter. 51/149: loss=0.12222745632547463, w0=0.037013509911432124, w1=0.005949374916225573\n",
      "GD iter. 52/149: loss=0.12205700204211617, w0=0.03740070525416055, w1=0.005962160052008997\n",
      "GD iter. 53/149: loss=0.12188916230995772, w0=0.03778447037535249, w1=0.005973966689919675\n",
      "GD iter. 54/149: loss=0.12172388358341758, w0=0.03816490317461581, w1=0.005984825755202827\n",
      "GD iter. 55/149: loss=0.12156111549357645, w0=0.038542094559889, w1=0.005994765900091053\n",
      "GD iter. 56/149: loss=0.12140081040663618, w0=0.03891612904270982, w1=0.0060038136979175035\n",
      "GD iter. 57/149: loss=0.12124292305438271, w0=0.03928708528192457, w1=0.006011993820411386\n",
      "GD iter. 58/149: loss=0.12108741022471997, w0=0.039655036580310735, w1=0.006019329199634692\n",
      "GD iter. 59/149: loss=0.12093423050232281, w0=0.04002005133819736, w1=0.006025841175892495\n",
      "GD iter. 60/149: loss=0.12078334405110858, w0=0.04038219346781348, w1=0.006031549632833536\n",
      "GD iter. 61/149: loss=0.12063471243160448, w0=0.04074152277177102, w1=0.006036473120852291\n",
      "GD iter. 62/149: loss=0.12048829844743605, w0=0.04109809528879325, w1=0.006040628969807288\n",
      "GD iter. 63/149: loss=0.12034406601612127, w0=0.04145196360952992, w1=0.006044033391982406\n",
      "GD iter. 64/149: loss=0.12020198006015352, w0=0.04180317716505362, w1=0.006046701576137507\n",
      "GD iter. 65/149: loss=0.12006200641502288, w0=0.04215178249040693, w1=0.006048647773421294\n",
      "GD iter. 66/149: loss=0.11992411175138094, w0=0.04249782346536433, w1=0.006049885375852252\n",
      "GD iter. 67/149: loss=0.11978826350901944, w0=0.04284134153438504, w1=0.006050426988012305\n",
      "GD iter. 68/149: loss=0.11965442984071692, w0=0.0431823759075616, w1=0.00605028449254186\n",
      "GD iter. 69/149: loss=0.11952257956433289, w0=0.043520963744212285, w1=0.00604946910997387\n",
      "GD iter. 70/149: loss=0.11939268212179599, w0=0.043857140320622746, w1=0.006047991453397895\n",
      "GD iter. 71/149: loss=0.11926470754385828, w0=0.04419093918331125, w1=0.0060458615784025295\n",
      "GD iter. 72/149: loss=0.11913862641967325, w0=0.04452239228907306, w1=0.006043089028705704\n",
      "GD iter. 73/149: loss=0.11901440987041363, w0=0.04485153013295036, w1=0.006039682877846792\n",
      "GD iter. 74/149: loss=0.11889202952627184, w0=0.04517838186517468, w1=0.006035651767282052\n",
      "GD iter. 75/149: loss=0.1187714575062986, w0=0.04550297539803802, w1=0.006031003941195291\n",
      "GD iter. 76/149: loss=0.11865266640062129, w0=0.04582533750356587, w1=0.006025747278308578\n",
      "GD iter. 77/149: loss=0.11853562925466324, w0=0.046145493902789575, w1=0.00601988932095312\n",
      "GD iter. 78/149: loss=0.11842031955504483, w0=0.04646346934734635, w1=0.006013437301637859\n",
      "GD iter. 79/149: loss=0.11830671121690224, w0=0.04677928769407202, w1=0.00600639816733274\n",
      "GD iter. 80/149: loss=0.11819477857240161, w0=0.047092971973193815, w1=0.00599877860166476\n",
      "GD iter. 81/149: loss=0.11808449636026476, w0=0.04740454445067809, w1=0.005990585045207743\n",
      "GD iter. 82/149: loss=0.11797583971615071, w0=0.047714026685239386, w1=0.005981823714031075\n",
      "GD iter. 83/149: loss=0.11786878416376571, w0=0.04802143958047351, w1=0.0059725006166583185\n",
      "GD iter. 84/149: loss=0.1177633056065927, w0=0.04832680343253723, w1=0.005962621569573488\n",
      "GD iter. 85/149: loss=0.11765938032015064, w0=0.04863013797376031, w1=0.0059521922114008806\n",
      "GD iter. 86/149: loss=0.11755698494470866, w0=0.048931462412542356, w1=0.005941218015873379\n",
      "GD iter. 87/149: loss=0.11745609647839177, w0=0.04923079546985618, w1=0.005929704303694199\n",
      "GD iter. 88/149: loss=0.11735669227062517, w0=0.04952815541265176, w1=0.0059176562533879516\n",
      "GD iter. 89/149: loss=0.11725875001587349, w0=0.049823560084428925, w1=0.005905078911228559\n",
      "GD iter. 90/149: loss=0.11716224774763771, w0=0.050117026933224215, w1=0.005891977200323982\n",
      "GD iter. 91/149: loss=0.11706716383267887, w0=0.05040857303723544, w1=0.005878355928930773\n",
      "GD iter. 92/149: loss=0.11697347696544197, w0=0.05069821512828865, w1=0.005864219798065141\n",
      "GD iter. 93/149: loss=0.11688116616265913, w0=0.05098596961333402, w1=0.005849573408471426\n",
      "GD iter. 94/149: loss=0.11679021075811238, w0=0.05127185259414132, w1=0.005834421267003597\n",
      "GD iter. 95/149: loss=0.11670059039754171, w0=0.051555879885350556, w1=0.00581876779247057\n",
      "GD iter. 96/149: loss=0.11661228503368427, w0=0.051838067031020126, w1=0.0058026173209917275\n",
      "GD iter. 97/149: loss=0.11652527492143405, w0=0.052118429319802234, w1=0.0057859741109050145\n",
      "GD iter. 98/149: loss=0.11643954061311286, w0=0.05239698179886432, w1=0.005768842347266281\n",
      "GD iter. 99/149: loss=0.11635506295384375, w0=0.05267373928666469, w1=0.005751226145975221\n",
      "GD iter. 100/149: loss=0.11627182307702116, w0=0.05294871638468143, w1=0.005733129557560168\n",
      "GD iter. 101/149: loss=0.11618980239987077, w0=0.05322192748818483, w1=0.0057145565706512216\n",
      "GD iter. 102/149: loss=0.11610898261909494, w0=0.0534933867961359, w1=0.005695511115168614\n",
      "GD iter. 103/149: loss=0.11602934570659866, w0=0.053763108320286336, w1=0.005675997065250903\n",
      "GD iter. 104/149: loss=0.11595087390529302, w0=0.05403110589354876, w1=0.005656018241945437\n",
      "GD iter. 105/149: loss=0.1158735497249718, w0=0.054297393177700055, w1=0.005635578415681592\n",
      "GD iter. 106/149: loss=0.11579735593825977, w0=0.0545619836704753, w1=0.005614681308545507\n",
      "GD iter. 107/149: loss=0.11572227557662838, w0=0.05482489071210463, w1=0.005593330596373421\n",
      "GD iter. 108/149: loss=0.11564829192647841, w0=0.05508612749134096, w1=0.005571529910679224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 109/149: loss=0.11557538852528586, w0=0.05534570705102231, w1=0.005549282840430491\n",
      "GD iter. 110/149: loss=0.11550354915781064, w0=0.055603642293208695, w1=0.005526592933686017\n",
      "GD iter. 111/149: loss=0.11543275785236541, w0=0.05585994598392998, w1=0.005503463699106767\n",
      "GD iter. 112/149: loss=0.11536299887714385, w0=0.05611463075757812, w1=0.005479898607351094\n",
      "GD iter. 113/149: loss=0.11529425673660583, w0=0.056367709120974135, w1=0.005455901092364148\n",
      "GD iter. 114/149: loss=0.11522651616792012, w0=0.05661919345713772, w1=0.005431474552570557\n",
      "GD iter. 115/149: loss=0.11515976213746121, w0=0.05686909602878472, w1=0.005406622351978639\n",
      "GD iter. 116/149: loss=0.11509397983736047, w0=0.057117428981575805, w1=0.005381347821203719\n",
      "GD iter. 117/149: loss=0.11502915468211089, w0=0.05736420434713748, w1=0.0053556542584174555\n",
      "GD iter. 118/149: loss=0.11496527230522288, w0=0.05760943404587471, w1=0.0053295449302294805\n",
      "GD iter. 119/149: loss=0.11490231855593183, w0=0.05785312988959291, w1=0.005303023072507118\n",
      "GD iter. 120/149: loss=0.1148402794959556, w0=0.05809530358394543, w1=0.00527609189113844\n",
      "GD iter. 121/149: loss=0.11477914139630109, w0=0.058335966730721216, w1=0.005248754562743473\n",
      "GD iter. 122/149: loss=0.11471889073411966, w0=0.05857513082998621, w1=0.005221014235337941\n",
      "GD iter. 123/149: loss=0.1146595141896097, w0=0.05881280728209065, w1=0.005192874028953554\n",
      "GD iter. 124/149: loss=0.11460099864296637, w0=0.0590490073895537, w1=0.005164337036218509\n",
      "GD iter. 125/149: loss=0.11454333117137747, w0=0.059283742358835496, w1=0.005135406322901549\n",
      "GD iter. 126/149: loss=0.1144864990460642, w0=0.05951702330200606, w1=0.005106084928422637\n",
      "GD iter. 127/149: loss=0.11443048972936727, w0=0.05974886123831968, w1=0.005076375866333033\n",
      "GD iter. 128/149: loss=0.11437529087187623, w0=0.0599792670957025, w1=0.005046282124767335\n",
      "GD iter. 129/149: loss=0.11432089030960238, w0=0.06020825171216058, w1=0.005015806666869801\n",
      "GD iter. 130/149: loss=0.11426727606119423, w0=0.06043582583711483, w1=0.004984952431197089\n",
      "GD iter. 131/149: loss=0.11421443632519498, w0=0.0606620001326689, w1=0.004953722332099367\n",
      "GD iter. 132/149: loss=0.11416235947734071, w0=0.060886785174815404, w1=0.004922119260081549\n",
      "GD iter. 133/149: loss=0.11411103406790006, w0=0.0611101914545855, w1=0.004890146082146306\n",
      "GD iter. 134/149: loss=0.1140604488190532, w0=0.06133222937914638, w1=0.004857805642120313\n",
      "GD iter. 135/149: loss=0.11401059262231036, w0=0.06155290927285079, w1=0.004825100760965107\n",
      "GD iter. 136/149: loss=0.11396145453596951, w0=0.06177224137824245, w1=0.004792034237073773\n",
      "GD iter. 137/149: loss=0.11391302378261182, w0=0.061990235857020765, w1=0.004758608846554616\n",
      "GD iter. 138/149: loss=0.1138652897466348, w0=0.06220690279096812, w1=0.004724827343502832\n",
      "GD iter. 139/149: loss=0.11381824197182294, w0=0.06242225218284252, w1=0.004690692460261131\n",
      "GD iter. 140/149: loss=0.1137718701589541, w0=0.06263629395723837, w1=0.004656206907670179\n",
      "GD iter. 141/149: loss=0.11372616416344267, w0=0.06284903796141769, w1=0.004621373375309644\n",
      "GD iter. 142/149: loss=0.11368111399301739, w0=0.06306049396611411, w1=0.004586194531730572\n",
      "GD iter. 143/149: loss=0.11363670980543465, w0=0.06327067166631158, w1=0.0045506730246797494\n",
      "GD iter. 144/149: loss=0.11359294190622578, w0=0.06347958068199967, w1=0.00451481148131666\n",
      "GD iter. 145/149: loss=0.11354980074647811, w0=0.06368723055890727, w1=0.004478612508423583\n",
      "GD iter. 146/149: loss=0.11350727692064973, w0=0.06389363076921592, w1=0.00444207869260934\n",
      "GD iter. 147/149: loss=0.11346536116441672, w0=0.06409879071225473, w1=0.004405212600507154\n",
      "GD iter. 148/149: loss=0.11342404435255274, w0=0.06430271971517756, w1=0.004368016778967037\n",
      "GD iter. 149/149: loss=0.11338331749684086, w0=0.0645054270336242, w1=0.0043304937552431\n"
     ]
    }
   ],
   "source": [
    "losses, ws = mean_squared_error_gd(y_tr, x, initial_w, max_iters=150, gamma=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02b2034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['350000' '-1']\n"
     ]
    }
   ],
   "source": [
    "best_w = ws[-1]\n",
    "pred = get_predictions(x_te, best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17327391",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"submission.csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx)).dot(tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64755be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y âˆˆ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating loss with sigmoid and using gradient descent\n",
    "def logistic_regression(y, tx, w):\n",
    "    loss = calculate_loss_lr(y, tx, w)\n",
    "    gradient = calculate_gradient_lr(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 50\n",
    "    threshold = 1e-5\n",
    "    lambda_ = 0.1\n",
    "    gamma = 1.\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and gradient descent on w.\n",
    "        loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "        w -= gamma * np.linalg.solve(hessian, gradient)\n",
    "        \n",
    "        print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcbbac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
