{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e616efb",
   "metadata": {},
   "source": [
    "# Project 1 - Team BAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c038e6",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ceffe021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import some libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b801ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452c81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (250000, 30)  y:  (250000,)\n"
     ]
    }
   ],
   "source": [
    "y = train[1]\n",
    "x_tr = train[2:].T\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74a654",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7588c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)  and y[:5]:  [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_tr = np.where(y == \"s\",1,0)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d204b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete features with more than 30% NaN values\n",
    "x_tr_prep = np.delete(x_tr, 0, 1) # infer this one later\n",
    "x_te_prep = np.delete(x_te, 0, 1) # infer this one later\n",
    "for i in [4, 4, 4, 9, 18, 18, 18, 18, 18, 18, 18]:\n",
    "    x_tr_prep = np.delete(x_tr_prep, i, 1)\n",
    "    x_te_prep = np.delete(x_te_prep, i, 1)\n",
    "x_tr_prep = x_tr_prep.astype(float)\n",
    "x_te_prep = x_te_prep.astype(float)\n",
    "x_tr_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4321ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x_tr_prep)\n",
    "x_te, _, _ = standardize(x_te_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef6c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 19), (568238, 19))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9135d0",
   "metadata": {},
   "source": [
    "## Step 3 - Implement ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15377ad",
   "metadata": {},
   "source": [
    "#### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43cf355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        # print w and loss\n",
    "        #print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90679f",
   "metadata": {},
   "source": [
    "#### Linear regression using stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9c7570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        #print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "         #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229356",
   "metadata": {},
   "source": [
    "#### Build polimonial basis function which can be used with Least Squares and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a593cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "        \n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x),1))\n",
    "    for j in range( 1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, j)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32f86e",
   "metadata": {},
   "source": [
    "#### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fe44b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    opt_weights = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    e = y - tx.dot(opt_weights)\n",
    "    mse = 1/(2*len(y)) * e.T.dot(e)\n",
    "    return opt_weights, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5430e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_ls(y, x, degrees=[1, 3, 7, 12]):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    ws = []\n",
    "    losses = []\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(x, degree)\n",
    "        w, loss = least_squares(y, tx)\n",
    "        rmse = np.sqrt(2 * loss)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(rmse)\n",
    "\n",
    "        #print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "         #     i=ind + 1, d=degree, loss=rmse))\n",
    "    ind = argmin(losses)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40409dda",
   "metadata": {},
   "source": [
    "#### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64755be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecad3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # TODO split and add test data\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(compute_rmse(y_tr, tx_tr, weight))\n",
    "        \n",
    "        #print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "         #      p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    ind = argmin(rmse_tr)\n",
    "    return ws[ind], losses[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b653c7",
   "metadata": {},
   "source": [
    "#### Logistic regression using gradient descent or SGD (y ∈ {0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0e3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating loss with sigmoid and using gradient descent\n",
    "def logistic_regression(y, tx, w):\n",
    "    loss = calculate_loss_lr(y, tx, w)\n",
    "    gradient = calculate_gradient_lr(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca38bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 500\n",
    "    threshold = 1e-8\n",
    "    gamma = .5\n",
    "    losses = []\n",
    "\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and gradient descent on w.\n",
    "        loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "        w -= gamma * np.linalg.solve(hessian, gradient)\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss_lr(y_tr, tx, w)))\n",
    "    \n",
    "    return w, losses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f0bae",
   "metadata": {},
   "source": [
    "#### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47b8018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_regularized_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 2000\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = np.zeros((tx.shape[1],), dtype=float)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3eec7",
   "metadata": {},
   "source": [
    "## Step 4 - Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c836471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x, best_w):\n",
    "    preds = x.dot(best_w).reshape((x.shape[0],))\n",
    "    y_te = np.where(preds < .5,-1,1)\n",
    "    y_pred = np.c_[test[0], y_te]\n",
    "    print(y_pred[0:5])\n",
    "    y_pred = np.insert(y_pred, 0, [\"Id\", \"Prediction\"], axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(pred, title=\"submission\"):\n",
    "    np.savetxt(title + \".csv\", pred, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x.shape[1],), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e218a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "MSE- GD Loss:  0.11338331749684086\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Gradient Descent\n",
    "w_gd, loss_gd = mean_squared_error_gd(y_tr, x, initial_w, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_gd)\n",
    "print(\"MSE - GD Loss: \", loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cbc6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '1']]\n",
      "MSE - SGD Loss:  0.11336016574552966\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error Stochastic Gradient Descent\n",
    "w_sgd, loss_sgd = mean_squared_error_sgd(y_tr, x, initial_w, max_iters=150, gamma=.005)\n",
    "pred = get_predictions(x_te, w_sgd)\n",
    "print(\"MSE - SGD Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "639ca13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n",
      "MSE - LS Loss:  0.11336016574552966\n"
     ]
    }
   ],
   "source": [
    "# Least Squares\n",
    "w_ls, loss_ls = least_squares(y_tr, x)\n",
    "#w_poly, loss_poly = polynomial_regression_ls(y_tr, x)\n",
    "pred = get_predictions(x_te, w_ls)\n",
    "print(\"MSE - LS Loss: \", loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06cc5f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['350000' '-1']\n",
      " ['350001' '-1']\n",
      " ['350002' '-1']\n",
      " ['350003' '-1']\n",
      " ['350004' '-1']]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "# TODO: add split data\n",
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "#w_rr, loss_rr = ridge_regression_demo(x, y, degree, split_ratio, seed)\n",
    "w_rr = ridge_regression(y_tr, x, .0005 )\n",
    "pred_rr = get_predictions(x_te, w_rr)\n",
    "#print(\"MSE - RR Loss: \", loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logictic Regression Gradient Descent\n",
    "w_logreg, loss_logreg = logistic_regression_gradient_descent(y_tr, x)\n",
    "pred_log = get_predictions(x_te, w_logreg)\n",
    "print(\"RMSE - LogRed Loss: \", loss_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression with GD\n",
    "w_reglog, loss_reglog = logistic_regression_regularized_gradient_descent(y_tr, x)\n",
    "pred_reglog = get_predictions(x_te, w_reglog)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
