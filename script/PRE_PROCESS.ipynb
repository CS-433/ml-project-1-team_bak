{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0291981",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from helpfulfun import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e35f2",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTAZIONE DEI MISSING VALUES\n",
    "# FEATURE STUDY (STUDIO DELLE VARIABILI)\n",
    "# DELETE OUTLIERS \n",
    "# STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d47949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(x_train, x_test, alpha=0):\n",
    "    \"\"\"\n",
    "    With this preprocess we are going to:\n",
    "    - impute missing values with the median\n",
    "    - perform feature study and remove variables that are useless\n",
    "    - impute outliers using alpha quantiles\n",
    "    - standardize data\n",
    "    \"\"\"\n",
    "    # Missing Values: \n",
    "    # prima di tutto cambia gli 0 nella variabile pri_jet_all_pt in -999 cosi da essere consideratoi outliers. Why? It represents       The scalar sum of the transverse momentum of all the jets of the events\n",
    "    # poi applica la funzione missing_values...vedi riga 91\n",
    "    \n",
    "    n_rows, n_cols = x_train.shape #N number of rows, D number of cols\n",
    "    missing_data = np.zeros(n_cols) #create a vector with len equal to the number of variables\n",
    "    missing_80 = [] # list indicating which columns we should remove\n",
    "    for regressor in range(n_cols):\n",
    "        missing_data[regressor] = np.count_nonzero(x_train[:,feature]==-999)/n_rows # compute percentage of missing values per column\n",
    "      \n",
    "        if missing_data[regressor]>0.8: \n",
    "            missing_80.append(regressor) # se maggiore di .8 allora aggiungere alla lista di colonne da rimuovere\n",
    "           \n",
    "        elif missing_data[regressor]>0: # se minore di .8 ma comunque con missing presenti: imputare missing con mediana!\n",
    "            impute_this = x_train[:,regressor] \n",
    "            median = np.median(impute_this[impute_this != -999])\n",
    "            x_train[:,feature] = np.where(x_train[:,feature]==-999, median, x_train[:,regressor])\n",
    "            x_test[:,feature] = np.where(x_test[:,feature]==-999, median, x_train[:,regreessor])\n",
    "                    \n",
    "    X[:,missing_80]=0\n",
    "    X_test[:,missing_80]=0\n",
    "    \n",
    "    # Feature study:\n",
    "    # Other trasformation for positive features\n",
    "    x_train, x_test = log_transf(x_train, x_test) #fa una trasformazione logaritmica\n",
    "    # Delete useless features\n",
    "    x_train = np.delete(x_train, [15,16,18,20], 1) # rimuove features inutili.. come lo decide quali sono inutili? Forse ci sono maggiori dettagli in exploration file \n",
    "    x_test = np.delete(x_test, [15,16,18,20], 1)\n",
    "    \n",
    "    # Delete outliers\n",
    "    x_train = outliers(x_train, alpha) # rimuove gli outliers giudicando i percentili\n",
    "    x_test = outliers(x_test, alpha)\n",
    "    \n",
    "    # Standardization\n",
    "    x_train, mean_x_train, std_x_train = standardize(x_train) # standardizza i dati.. come abiam fatto noi\n",
    "    x_test, _, _ = standardize(x_test, mean_x_train, std_x_train)\n",
    "    \n",
    "    \n",
    "     \n",
    "    return x_train, x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27a83c",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import the datasets\n",
    "\n",
    "train_list = np.genfromtxt(\"train.csv\", dtype=None, delimiter=\",\", skip_header =0, unpack=True, encoding=None)\n",
    "train = np.array(train_list)\n",
    "\n",
    "test_list = np.genfromtxt(\"test.csv\", dtype=None, delimiter=\",\", skip_header =1, unpack=True, encoding=None)\n",
    "test = np.array(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d107229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = test[2:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[1, 1:]\n",
    "x_tr = train[2:,1: ].T.astype('float')\n",
    "print(\"x: \", x_tr.shape, \" y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = np.where(y == \"s\",1,-1)\n",
    "print(y_tr.shape, \" and y[:5]: \", y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr[:,22].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7ad80",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58884f87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(x_tr.shape[1] ):\n",
    "    print(i, np.count_nonzero(abs(x_tr[:,i]) == 999.0), train[i+2, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep all except for those with many missng\n",
    "x_tr_2 = np.array(np.delete(x_tr,(4,5,6,12,26,27,28), axis = 1))\n",
    "train = np.array(train_list)\n",
    "train = np.delete(train, (6,7,8,14,28,29,31), axis= 0)\n",
    "for i in range(x_tr_2.shape[1] ):\n",
    "    print(i, np.count_nonzero(abs(x_tr_2[:,i]) == 999.0), train[i+2,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute using median\n",
    "x_tr_2[abs(x_tr_2)==999.0] = np.nan\n",
    "ind = np.where(np.isnan(x_tr_2))\n",
    "\n",
    "median = np.nanmedian(x_tr_2, axis=0, out=None, overwrite_input=False)\n",
    "\n",
    "x_tr_2[ind] =  np.take(median, ind[1])\n",
    "\n",
    "x_tr_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(x_tr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x_te_2\n",
    "x_te_2 = np.array(np.delete(x_te,(4,5,6,12,26,27,28), axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the dataset:\n",
    "np.savetxt(\"x_tr_2.csv\", x_tr_2, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"x_te_2.csv\", x_te_2, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove the primitive variables:\n",
    "x_tr_3 = x_tr_2[:,:9]\n",
    "x_tr_3.shape\n",
    "for i in range(x_tr_3.shape[1] ):\n",
    "    print(i, np.count_nonzero(abs(x_tr_3[:,i]) == 999.0), train[i+2,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5632e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now impute outliers\n",
    "\n",
    "def impute_outliers_mean(a):\n",
    "    '''impute outliers with mean'''\n",
    "    # 1. Determine mean and standard deviation\n",
    "    mean = np.mean(a, axis = 0)\n",
    "    std_dev = np.std(a, axis = 0)\n",
    "    # 2. Normalize array around 0\n",
    "    zero_based = abs(a - mean)\n",
    "    # 3. Define maximum number of standard deviations\n",
    "    max_deviations = 2\n",
    "    # 4. Access only non-outliers using Boolean Indexing\n",
    "    num_out = 0\n",
    "    for i in range(a.shape[1]):\n",
    "        for j in range(a.shape[0]):\n",
    "            if zero_based[j,i] > max_deviations * std_dev[i]:\n",
    "                a[j,i] = mean[i]\n",
    "                num_out += 1\n",
    "    print(num_out, \"observations have been replaced by the corresponding mean\")\n",
    "    \n",
    "    return a\n",
    "\n",
    "#x_tr_3_no_out = impute_outliers_mean(x_tr_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaeefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_3 = x_te_2[:,:9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970264fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the dataset:\n",
    "np.savetxt(\"x_tr_3_no_out.csv\", x_tr_3_no_out, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"x_te_3_no_out.csv\", x_te_3, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_4 = np.array(np.delete(x_tr,(0,5,6,9,21,23,29), axis = 1))\n",
    "x_te_4 = np.array(np.delete(x_te,(0,5,6,9,21,23,29), axis = 1))\n",
    "train = np.array(train_list)\n",
    "train = np.delete(train, (2,7,8,11,23,25,31), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412db103",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x_tr_4.shape[1] ):\n",
    "    print(i, np.count_nonzero(abs(x_tr[:,i]) == 999.0), train[i+2, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute nan using median\n",
    "x_tr_4[abs(x_tr_4)==999.0] = np.nan\n",
    "ind = np.where(np.isnan(x_tr_4))\n",
    "\n",
    "median = np.nanmedian(x_tr_4, axis=0, out=None, overwrite_input=False)\n",
    "\n",
    "x_tr_4[ind] =  np.take(median, ind[1])\n",
    "\n",
    "x_tr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed860752",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_4_no_out = impute_outliers_mean(x_tr_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"x_tr_4_no_out.csv\", x_tr_4_no_out, fmt=\"%s\", delimiter=\",\")\n",
    "np.savetxt(\"x_te_4_no_out.csv\", x_te_4, fmt=\"%s\", delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
